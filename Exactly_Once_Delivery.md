## âš ï¸ Problem Without Exactly-Once Processing


### Setup:
ğŸ—‚ï¸ `Topic:` order-topic  
ğŸ“Š `Partitions:` P0, P1

ğŸ“¤ `Producer sends orders like:`
- ğŸ”¹ P0: OrderID: 101, Amount: $50 (M1)
- ğŸ”¹ P1: OrderID: 102, Amount: $75 (M2)

ğŸ“¥ `Consumer reads orders and updates database or sends emails.`


### Problems that can happen:

#### ğŸ” Duplicate Processing
- âš ï¸ `Producer sends message` but `doesnâ€™t get acknowledgment due to network issues`.
   - ğŸ“¡ Internally
     - ğŸ“¤  Producer sends a message to Kafka.
     - ğŸ—„ï¸ `Kafka stores` the `message safely.`
     - ğŸ“¨ `Kafka sends back a message` called `an acknowledgment (ack)` saying, â€œHey producer, I got your message!â€
     - âš ï¸  If the `producer does NOT receive` this ack because:
          - ğŸŒ `Network issue` `between Kafka and producer`, or
          - ğŸ¢  Producer is `slow or busy` and `misses the ack,`
     - â“  Then the `producer thinks` the `message didnâ€™t get through.`
     - ğŸ”„  So, the `producer sends the same message again` to `be safe.`

- ğŸ”„ `Producer resends` the `same message`.
- ğŸ—ƒï¸ `Kafka ends up with duplicate messages:`
    - ğŸ“ P0: M1, M1
    - ğŸ“ P1: M2, M2
- ğŸ” `Consumer processes` the `same order twice.`

ğŸ’¥ `Result: Customer is charged twice or gets duplicate emails.`

#### ğŸš« Message Loss
- ğŸ•µï¸ `Consumer reads message` and `commits offset before processing`.
- ğŸ’¥ `Consumer crashes before updating database.`
- â© `After restart`, `consumer skips that message` (because offset was committed).

âŒ `Result: The message is lost and the order is never processed.`

---

# ğŸ¯ Why Exactly-Once matters?

To guarantee:

"`Every message` is `delivered and processed only once`, even with `failures`, `retries`, and `crashes`."

It Solves Two Big Problems:
- âœ… ğŸ›‘ No duplicates.  
- âœ… ğŸ“­ No message loss.

---

## âš™ï¸ How to Use Exactly-Once in Kafka

Kafka has a built-in mechanism for **exactly-once semantics (EOS)** using:

- âœ… **Idempotent Producers**
- ğŸ”„ **Transactions**
- âš™ï¸ **Idempotent Consumers**


## ğŸ› ï¸ You Need to:

1. âœ… **Enable Idempotence** on the producer.
2. ğŸ”’ **Use Kafka Transactions** to group writes and commits.
3. ğŸ“¥ **Use transactional-aware consumers.**

---

## âœ”ï¸ Pros and Cons

### âœ… Pros

- ğŸ§¼ No duplicates
- ğŸ›¡ï¸ No lost messages
- âš›ï¸ Atomic writes to multiple topics
- ğŸ”— Clean integration with external systems (like DBs)

### âŒ Cons

- ğŸ•’ More latency due to transactions
- âš™ï¸ Slightly more complex setup
- ğŸ§  Higher resource usage
- ğŸ§¬ Needs Kafka 0.11+ and Kafka client config

---

## âš™ï¸ When is Exactly-Once Useful?

### ğŸ’¸ 1. Payment Systems

**Example:** ğŸ’³ A user clicks â€œPay Nowâ€ once.
- **Why Exactly-Once?**  
   - âœ… ğŸ§¾ To avoid double charging the customer.
   - âœ… ğŸ§ƒ To make sure the payment is not lost.


### ğŸ¦ 2. Banking & Financial Transactions

**Example:** ğŸ§ Money is transferred between accounts.
- **Why Exactly-Once?**  
   - âœ… ğŸ” To prevent duplicate transfers or missing funds.
   - âœ… ğŸ§® Guarantees account balances stay accurate.


### ğŸ›’ 3. Order Processing in E-commerce

**Example:** ğŸ“¥ User places an order for a product.
- **Why Exactly-Once?**  
   - âœ… ğŸ“¦ Avoid sending duplicate orders to the warehouse.
   - âœ… ğŸ§º Ensure the order is never missed.



### ğŸ“¦ 4. Inventory Management

**Example:** ğŸ§¾ Updating stock count after a sale.
- **Why Exactly-Once?**  
   - âœ… ğŸš« Prevents over-selling or wrong stock levels.
   - âœ… ğŸ“Š Keeps inventory accurate and up-to-date.


### ğŸ“Š 5. Analytics / Event Tracking

**Example:** ğŸ–±ï¸ Logging user actions or clicks.
- **Why Exactly-Once?**  
   - âœ… ğŸ“ˆ Avoid inflated numbers from duplicates.
   - âœ… ğŸ’¥ Prevent missing data from crashes.


### ğŸ” 6. Data Pipelines / ETL Jobs

**Example:** ğŸ”„ Moving data between systems (Kafka â†’ Database).
- **Why Exactly-Once?**  
   - âœ… ğŸ§· Ensures data is not duplicated or lost during transfers.
   - âœ… ğŸ—ƒï¸ Keeps data consistency across systems.

---


# ğŸš€ EOS (Exactly Once Semantics) 

**What it means:**  
Kafka guarantees that `each message is processed` `only once`, even with:

- ğŸ”„ Producer retries
- ğŸ” Consumer restarts
- ğŸ’¥ Crashes

It combines all of the above:
- âœ… Idempotent Producers
- âœ… Kafka Transactions
- âœ… Idempotent Consumers  

---


# ğŸ”„ Kafka Idempotent Producer

## â“ What is an Idempotent Producer?

A Kafka producer sends messages to Kafka topics.

Sometimes, `due to network issues or failures`, it `may retry sending` the `same message`.

This can cause duplicate messages in Kafka.

ğŸ’¡ **Idempotent Producer** makes sure even if the producer retries, `Kafka stores` the `message only once`.

## ğŸ§  Why is it Needed?

Imagine:

ğŸ›’ You place an order â†’ "`order-123`"  
ğŸ“¤ Producer sends the message to Kafka  
ğŸ’¾ Kafka stores it, but the producer doesnâ€™t get an acknowledgment (ack)  
ğŸ” Producer thinks the message failed and sends it again

âŒ Without idempotence â†’ Duplicate message in Kafka  
âœ… With idempotence â†’ Only one message is stored


## âš™ï¸ How Does It Work Internally?

Kafka uses two things to detect duplicates:

### 1ï¸âƒ£ PID (Producer ID)

ğŸ†” A `unique ID` `given by Kafka broker` to `each producer instance` `when it starts`.

`Identifies` who is sending the message.

### 2ï¸âƒ£ Sequence Number

ğŸ”¢ `Assigned by the producer` itself (not Kafka).  
Itâ€™s like a message counter per partition.

For example:

```bash

toipc : order-topic
Partition 0:  
            M1 -> Seq 1
            M2 -> Seq 2
            M3 -> Seq 3
Partition 1:
            M4 -> Seq 1
            M5 -> Seq 2
Partition 2:
            M6 -> Seq 1
            M7 -> Seq 2
            M8 -> Seq 3            
```


ğŸ§© Sequence numbers are maintained independently per partition  
ğŸ”¹ The producer assigns these sequence numbers when sending

## Kafka tracks:  
ğŸ§¾ `Producer ID + Partition + Sequence Number`
- If it `sees the same combination again` â†’ `it's a duplicate` â†’ `Kafka ignores it`.


## ğŸ” How Retries Work (with idempotence)

1. Producer sends message with seq number = 3 of Message 8 in Partition 2
2. Kafka stores it and sends ack
3. If ack is lost, producer retries
4. Retry is sent with the same sequence number (3)
5. Kafka sees itâ€™s already stored seq 3 from this producer â†’ skips it

âœ… No duplicate in the topic.

## ğŸ§  Key Points

- ğŸ“¦ `Each partition` has `its own sequence number counter`
- â±ï¸ Sequence number starts from 1 (or 0) and `increments only` `after receiving` an acknowledgment `ack` from the `Kafka broker`.
- ğŸ” If `a message fails` and is `retried`, the `same sequence number` is `used again`
- ğŸ” Kafka checks (PID + Partition + Seq) to detect duplicates.


## â“ Who adds the sequence number in Kafka producer?

ğŸ› ï¸ **Answer:**

- The `Kafka client library` (the Kafka Producer inside your app) `automatically assigns` the sequence number for each message you send â€” you donâ€™t add it manually. 
- ğŸ§° When you call `kafkaTemplate.send(...)`, the KafkaTemplate uses Kafka's producer client under the hood. 
- âš™ï¸ The Kafka producer client handles sequence numbers internally.

## â“ Do I need to manually add PID to my producer?

- ğŸš« No, `Kafka broker handles` this `automatically` if you enable idempotence.
- âœ… You just turn on a setting, and Kafka does the rest.  
- ğŸ” You donâ€™t manually set PID.


## âš™ï¸ How to enable idempotent producer in Spring Boot?

If you're using KafkaTemplate, you just need to set this property in your `application.yml` or `application.properties`.

```bash

spring:
  kafka:
    producer:
      properties:
        acks: all
        retries : 3 
        enable.idempotence: true

```

---

## ğŸ”„ Quick Recap of Your Scenario:

- ğŸ§‘â€ğŸ’» Producer PID = 123
- ğŸ“¤ It sends a message â†’ Kafka assigns sequence number = 3 of message 8 in Partition 2
- ğŸ“¬ Kafka stores the message and sends an ack
- âŒ But the ack is lost (network issue)
- ğŸ” Producer retries sending the same message (because it didn't get ack)

ğŸ‘‰ **Your question:**

â“ Wonâ€™t the producer assign sequence number = 4 now and Kafka think itâ€™s a new message?

ğŸ§° kafka producer client library handles:
- Sequence number for each partition

ğŸ“¡ Kafka broker handles only:
- PID
- Receives messages
- Tracks the producer ID + sequence number for each partition
- Detects duplicates based on those


## ğŸ§‘â€ğŸ’» On Producer Side (PID 123):

- ğŸ§® Keeps a sequence number counter per partition
- For partition 3, sends message 8 (M8) with seq = 3
- ğŸ• Waits for ack
- âŒ Now, if the ack is lost, the producer does not increase the sequence number.
- ğŸ” Instead, it retries sending the same message with the same seq = 3.

ğŸ“¡ Kafka sees:

- Producer ID = 123
- Partition = 2
- Sequence number = 3 (same as before)

ğŸ§  Kafka checks its internal state:

ğŸ—‚ï¸ "I've already received a message from PID 123 for partition 2 with sequence 3"

ğŸ‘‰ Duplicate detected â†’ Kafka ignores it

âœ… **Result: No duplicate message in topic**


## ğŸ” And when does the producer increase the sequence number?

â© **Only after Kafka sends a successful ack.**


## ğŸ’¥ What if Producer crashes and restarts?

- â™»ï¸ Kafka gives it a new PID
- ğŸ”¢ Sequence number starts from 0
- ğŸ” Kafka can detect this as a new session, and handle it properly


## ğŸ“Š Step Table

| ğŸªœ Step                          | ğŸ‘· Who Does It?                 | â— Why it matters                                 |
|-------------------------------|-------------------------------|--------------------------------------------------|
| ğŸ†” Assign producer ID (PID)     | ğŸ–¥ï¸ Kafka broker                | Identifies unique producer instance              |
| ğŸ”¢ Assign sequence numbers      | ğŸ§‘â€ğŸ’» Kafka producer client (library) | Helps Kafka detect duplicates                    |
| ğŸ§¾ Detect duplicates            | ğŸ–¥ï¸ Kafka broker                | Ensures exactly-once delivery if idempotence is enabled |
| âœï¸ You add messages to topic     | ğŸ’» Your Spring Boot app (KafkaTemplate) |                                                  |


---


# ğŸ§¾ Kafka Transactions

## â“ What is a Transaction?

- ğŸ§© A transaction `groups multiple Kafka operations` to be `treated` as `one atomic unit`. 
- âœ… Either `all messages succeed` or `none are saved`. 
- ğŸ¯ Helps `achieve exactly-once delivery` `when producing multiple messages`.

## ğŸ¤” Why do we need Transactions?

ğŸ“¦ **Example:**

You have an order system that:

- ğŸ“ Writes a message to the `order-topic` (new order placed)
- ğŸ’³ Writes another message to the `payment-topic` (payment info)

âŒ If one message is stored but the other fails, your system gets out of sync!

ğŸ” **Transactions help avoid this problem** by making sure both messages are saved atomically â€” together or none at all.


## ğŸ”‘ How Transactions Work in Kafka

1. â–¶ï¸ Producer starts a transaction
2. âœ‰ï¸ Producer sends multiple messages (to one or more topics/partitions)
3. âœ… Producer commits the transaction â€” all messages become visible atomically
4. âŒ Or aborts the transaction â€” none of the messages are saved

```bash

Topics:
order-topic -> Partition 0 -> Order message -1001
payment-topic -> Partition 1 -> Payment message -1001
```


### ğŸ§ª Scenario:

- ğŸŸ¢ Producer starts transaction
- ğŸ“¤ Sends `order-1001` to `order-topic` topic partition 0
- ğŸ“¤ Sends `payment-1001` to `payment-topic` topic partition 1
- âœ… Producer commits transaction

ğŸ“Œ **Result:** Both messages appear in their topics at the same time.

âŒ **If failure happens before commit:**

- ğŸš« Producer aborts transaction
- ğŸ•³ï¸ Neither message appears in Kafka

## ğŸ§° Use Cases for Kafka Transactions

- ğŸ”— **Atomic writes across multiple topics**  
   - Ensuring messages sent to different topics succeed or fail together (e.g., orders and payments topics).

- ğŸ§µ **Atomic writes across multiple partitions within the same topic**  
   - Ensuring messages to different partitions of one topic are committed together to keep data consistent.

- â™»ï¸ **Exactly-once stream processing**  
  - **Example:**  
    - A stream processing app reads from input topics, processes data, and writes to output topics.  
  - **Problem:** 
    - Duplicates or partial writes cause inconsistent downstream data.  
  - **Solution:** 
    - Wrap the entire processing and output writes inside a transaction so that results are committed atomically.

- ğŸ“¡ **Distributed event coordination**  
   - Ensuring multiple related events sent to different topics or partitions are published together or not at all, maintaining system consistency.


## ğŸ› ï¸ How to Use Transactions in Kafka Producer

| ğŸªœ Step             | ğŸ§‘â€ğŸ’» Kafka Producer (API)         |
|-------------------|------------------------------|
| â–¶ï¸ Start           | `producer.beginTransaction()` |
| âœ‰ï¸ Send Messages   | `producer.send(record)`       |
| âœ… Commit          | `producer.commitTransaction()` |
| âŒ Abort (on failure) | `producer.abortTransaction()`  |

## ğŸ›¡ï¸ Kafka Consumer Isolation Level: `read_committed`

- **ğŸ¯ Purpose**
   - Ensure that Kafka consumers only read `committed messages`, even if a producer fails or crashes mid-transaction.
- **ğŸ§  Concept**
   - Kafka supports `transactions` to `write multiple records atomically`. If `a producer fails` `before completing a transaction`, `those partial records` `should not` be `visible to consumers`.
- By default, `consumers might read` `these uncommitted messages` `unless configured properly`.


## ğŸ”’ Solution: `read_committed` Isolation

### âœ… With `read_committed`
- Consumers `only read committed messages`
- Uncommitted or in-progress messages are `hidden`
- Ensures `exactly-once semantics` (EOS)

### âš ï¸ Without it (default: `read_uncommitted`)
- Consumers might read `uncommitted` or `partial` transactions
- Risk of `dirty reads`, duplicates, or `inconsistent state`

## âš™ï¸ How to Configure
- In your **consumer application**, set the following in `application.properties` (or YAML equivalent):

```properties
spring.kafka.consumer.properties.isolation.level=read_committed

```

## ğŸ§ª How to Use Transactions in Spring Boot KafkaTemplate


- ğŸ§ª Setting `transaction-id-prefix` `automatically enables transaction support` in KafkaTemplate. 
- ğŸ” Use `KafkaTemplate.executeInTransaction(...)` to `send messages transactionally`.
- add isolation level in `consumer side` as `read_committed`


ğŸ§° Enable Transactions in application.yml or application.properties
```bash


spring:
  kafka:
    producer:
      transaction-id-prefix: txn-       # ğŸ›¡ï¸ Enables transactional support for KafkaTemplate
    listener:
      ack-mode: RECORD                  # ğŸ“ Ensures proper message acknowledgment for transactions
    consumer:
      isolation-level: read_committed   # ğŸ”’ Reads only committed (non-aborted) messages
      
```
ğŸ§° Spring Kafka simplifies transactions with `executeInTransaction` method.
```bash

kafkaTemplate.executeInTransaction(kt -> {
    kt.send("orders", orderKey, orderValue);
    kt.send("payments", paymentKey, paymentValue);
    return true; // commits transaction if no exceptions
});
```

- â–¶ï¸ `executeInTransaction` starts the transaction
- ğŸ§¾ Runs your send logic
- âœ… Commits automatically if no exception
- âŒ If an exception happens, transaction aborts automatically

### ğŸ” Return Value Matrix

| ğŸ§¾ Return Value | âœ… No Exception               | âŒ Exception Thrown              |
|----------------|------------------------------|----------------------------------|
| `true`         | Transaction committed automatically | Transaction rolled back automatically |
| `false`        | Transaction rolled back automatically | Transaction rolled back automatically |



## ğŸ”„ Step Comparison Table

| ğŸªœ Step               | ğŸ§‘â€ğŸ’» KafkaProducer       | ğŸ’» KafkaTemplate (Spring Kafka)   |
|----------------------|-------------------------|----------------------------------|
| â–¶ï¸ Start transaction  | `beginTransaction()`     | `executeInTransaction()`         |
| âœ‰ï¸ Send messages      | `send(...)`              | `send(...)` inside lambda        |
| âœ… Commit transaction | `commitTransaction()`    | Automatic if no exception        |
| âŒ Abort transaction  | `abortTransaction()`     | Automatic if exception thrown    |


## ğŸ§  Key Points

- ğŸ§± Transactions ensure **atomicity**: all messages inside either get committed or none.
- ğŸ§© Used to avoid partial updates when producing messages to multiple topics/partitions.
- âš™ï¸ Requires producer config `transactional.id` or `transaction-id-prefix` in Spring.
- ğŸ”„ Transactions work together with **idempotent producers** for exactly-once semantics.

---

## ğŸ”„ Kafka Transactions and Idempotent Producer

### 1. Idempotent Producer and Transactions Relationship

- âš™ï¸ `Transactions` `require` `idempotent producer to work`. 
- Setting `transactional.id` (or `transaction-id-prefix` in Spring Boot) enables transactions and automatically enables idempotence (`enable.idempotence=true`).


### ğŸ“Œ What happens under the hood:

When you set `transactional.id` or `transaction-id-prefix`
  - â¡ï¸ Kafka automatically enables idempotence (`enable.idempotence=true`).

### ğŸ” Why is this required?

Because transactions and idempotence work together to achieve exactly-once semantics:

| ğŸ·ï¸ Feature       | ğŸ¯ Purpose                             |
|------------------|--------------------------------------|
| ğŸ” Idempotence    | Prevents duplicate messages on retries |
| ğŸ”’ Transactions   | Groups messages atomically (all or none) |



---


# ğŸ” Retryable & Idempotent Consumers

## 1ï¸âƒ£ What is the Problem?

- ğŸ§  When a Kafka consumer reads a message and starts processing it, failures can happen â€” due to:
   - âŒ Errors 
   - ğŸ’¥ App crashes 
   - ğŸ’¤ Downtime 
   - ğŸ Bugs
- ğŸ” If the consumer retries the same message after a failure, it might process it more than once â€” which can lead to:
   - ğŸ“› Duplicate operations 
   - âš ï¸ Inconsistent state


## ğŸ§µ Scenario Breakdown

ğŸ“¦ **Topic:** `order-topic`  
ğŸ§¾ **Message:** `order-123`

### âœ… Step-by-step:

1. 1ï¸âƒ£ A consumer reads the message `order-123` from `order-topic`. 
2. 2ï¸âƒ£ It starts processing the message:
    - ğŸ—ƒï¸ âœ… Updates the inventory database
    - ğŸŒ âœ… Calls an external API
3. 3ï¸âƒ£ ğŸ’¥ But before it can commit the offset back to Kafka, the consumer crashes or fails.


## âš ï¸ The Core Issue

- ğŸ•³ï¸ Since Kafka didnâ€™t get the offset commit, it assumes the message wasnâ€™t processed â€”  
- ğŸ§  But in reality, it was â€” just not acknowledged.

ğŸ“‰ This leads to the same message being retried, and unless the processing is idempotent, it causes:
   - ğŸ” Duplicate executions 
   - ğŸ§Ÿ Data inconsistencies 
   - ğŸš¨ Business logic failures

## ğŸ” What Happens Next:

- â™»ï¸ The consumer restarts and reads the same message (`order-123`) again from Kafka.
- But:
   - ğŸ—ƒï¸ The DB was already updated 
   - ğŸŒ The API was already called
- ğŸ“› So, the message gets processed again, causing problems like:
   - âŒ Inventory reduced twice 
   - âŒ Duplicate API calls 
   - âŒ Wrong or inconsistent state

---

# ğŸ”„ What is a Retryable Consumer?

A `Retryable Consumer` is a Kafka consumer that can `automatically retry processing a message` when `it fails`, without `crashing or re-reading the message from Kafka` from the beginning.

## ğŸ“š Example:

- ğŸ—‚ï¸ Topic: `order-topic`
- ğŸ“Œ Partition 0: `{ "orderId": "123", "item": "Laptop", "price": 1000 }`

Consumer reads the message from `order-topic` and tries to process it (e.g., store it in DB), but the database is down.

#### ğŸ’¥ What happens with a normal consumer?

- Crash ğŸ’¥
- ğŸ›‘ Block / Stop consuming
   - The consumer keeps trying to process the same failed message again and again. 
   - It does not move on to the next message, because the offset is not committed. 
   - So it blocks the partition â†’ no progress.

## ğŸ¤– What a Retryable Consumer does smarter:

- ğŸ”„ Tries to process the message
- âŒ If it fails, it moves the message to a retry topic
- â³ Waits (e.g., 10s, 1m, etc.) based on the delay time that we mentioned
- ğŸ” Tries again later
- ğŸ§¾ Keeps track of retries
- ğŸš« Gives up and moves it to a dead-letter-topic (DLT) after too many failures


## ğŸ’¡ Why use a Retryable Consumer?

- Because things break:
   - ğŸŒ Network glitches
   - ğŸ’¾ Temporary DB failure 
   - â³ Third-party API timeout

- A retryable consumer helps:
   - ğŸ“ˆ Improve reliability 
   - âš™ï¸ Keep processing other messages 
   - ğŸ§¯ Handle errors gracefully 
   - ğŸ›¡ï¸ Handle temporary errors without crashing or blocking processing

## ğŸ“… When to use it?

- Use a Retryable Consumer when:
  - âš ï¸ `Your message processing` is `not 100% reliable`
  - ğŸ”„ `You want to retry on transient errors` (e.g., DB down, API timeout)
  - ğŸš« You donâ€™t want to crash or block your Kafka consumer

## âš ï¸ The duplicate update problem on retries

- When `your consumer retries` processing the `same message multiple times` (because the first attempts failed), and the processing involves something like:
  - ğŸ—ƒï¸ Updating a database 
  - ğŸ§¾ Creating an order record 
  - ğŸ“§ Sending an email 
  - ğŸ’³ Charging a payment

- then `each retry` might do the same action again, `causing duplicate effects` like:
  - ğŸ›‘ Duplicate rows in DB 
  - ğŸ’¸ Duplicate charges 
  - ğŸ“¨ Multiple emails sent

## â“ Why does this happen?

- Because Kafka retries mean:
   - â›” Message processing fails â†’ retry â†’ process the same message again
- If `your processing logic is not idempotent`, it `repeats side effects`.

## ğŸ” Two Types of Retries

### 1. 1ï¸âƒ£ Synchronous (in-memory) retries
   - â±ï¸ `Happens immediately` `inside the consumer`
   - ğŸ”„ Retries a few times (e.g. 3 attempts with 5s backoff)
   - âš ï¸ `If it still fails` â†’ `escalate` to `async retries`
- âŒ **Problem:** Blocks that Kafka partition while retrying!

---

### 2. 2ï¸âƒ£ Asynchronous retries using retry topic (e.g. `order-topic-retry`)
   - ğŸ“¤ `Pushes failed message` to a `retry topic`
   - ğŸ‘¥ `A separate consumer` processes it later (after delay)
   - ğŸ†“ `Original consumer` is `free` to continue
- âœ… **Scales better, doesn't block partitions**

## ğŸ”„ Retry topic flow

```bash

1. order-topic  
    â†“  
[Main Consumer tries to process]  
    â†“ (fails after in-memory retries)  
2. order-topic-retry  
    â†“ (processed by separate retry consumer)  
[Retry Consumer tries again]  
    â†“ (fails again)  
3. order-topic-dlt (Dead Letter Topic)

```
## âš™ï¸ Two Ways to Implement Retry Mechanism


### 1ï¸âƒ£ Manual Implementation (Recommended Approach)

- ğŸ› ï¸ `Manually send message` `after in-memory retries fail` to `a separate retry consumer class`
- ğŸ”„ `Retry consumer` tries processing the message
- ğŸš¨ If `retry consumer fails again`, `send message` to `dead letter consumer class` for handling

- How to handle these manually

   - ğŸ”„ Implement `in-memory retry loop` `inside the consumer method`. 
   - ğŸ“¤ If retries fail, `send a custom error message` to a `retry topic` with exception info. 
   - ğŸ‘¥ Have `a separate retry consumer` that reads the retry topic, processes the message, and if `it fails again`, `sends it to the Dead Letter Topic (DLT)` with error details. 
   - ğŸ“ Use `manual offset commit` with `Acknowledgment.acknowledge()` after successful processing to control message acknowledgment precisely. 
   - âš™ï¸ Provides **fine-grained control** over retry logic, error handling, and offset management. 
   - ğŸ›¡ï¸ Enables **integration with Kafka transactions** for exactly-once semantics if needed.

``` bash


`Main Consumer` (in-memory retries) (order-topic) :
Reads from order-topic, retries 3 times in memory, then pushes to order-topic-retry.

@Service
public class OrderConsumer {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    private static final int MAX_RETRIES = 3;

    @KafkaListener(topics = "order-topic", groupId = "order-group")
    public void consume(
        @Payload String message,
        Acknowledgment acknowledgment
    ) {
        int attempts = 0;
        boolean success = false;

        while (attempts < MAX_RETRIES) {
            try {
                System.out.println("Processing order: " + message);
                // Simulate processing
                processOrder(message);
                acknowledgment.acknowledge(); // âœ… Commit offset only after success
                success = true;
                break;
            } catch (Exception e) {
                attempts++;
                System.out.println("Failed attempt " + attempts + " for message: " + message);
                try {
                    Thread.sleep(2000); // Wait before retry
                } catch (InterruptedException ignored) {}
            }
        }

        if (!success) {
            // Send to retry topic
            kafkaTemplate.send("order-topic-retry", message);
        }
    }

    private void processOrder(String message) {
        if (message.contains("fail")) {
            throw new RuntimeException("Simulated failure");
        }
        // Simulate order DB save
        System.out.println("âœ… Order processed: " + message);
    }
}


`Retry Consumer` (order-topic-retry) :
If the message still fails here â†’ send it to order-topic-dlt.

@Service
public class RetryConsumer {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    private static final int MAX_RETRY_ATTEMPTS = 2;

    @KafkaListener(topics = "order-topic-retry", groupId = "order-retry-group")
    public void retryConsume(
        @Payload String message,
        Acknowledgment acknowledgment
    ) {
        int attempts = 0;
        boolean success = false;

        while (attempts < MAX_RETRY_ATTEMPTS) {
            try {
                System.out.println("ğŸ” Retrying order: " + message);
                processOrder(message);
                acknowledgment.acknowledge();
                success = true;
                break;
            } catch (Exception e) {
                attempts++;
                System.out.println("âŒ Retry attempt " + attempts + " failed for: " + message);
                try {
                    Thread.sleep(3000);
                } catch (InterruptedException ignored) {}
            }
        }

        if (!success) {
            // Send to dead letter topic
            kafkaTemplate.send("order-topic-dlt", message);
        }
    }

    private void processOrder(String message) {
        if (message.contains("fail")) {
            throw new RuntimeException("Still failing in retry");
        }
        System.out.println("âœ… Retry processed: " + message);
    }
}

`Dead Letter Consumer` (DLT) (order-topic-dlt)

This consumer handles messages that failed all retries

@Service
public class DeadLetterConsumer {

    @KafkaListener(topics = "order-topic-dlt", groupId = "dlt-group")
    public void handleDeadLetter(@Payload String message) {
        System.out.println("ğŸš¨ Dead letter message received: " + message);
        // Notify team, alert, save to DB, etc.
    }
}



```


## 2ï¸âƒ£ Using `@RetryableTopic` Annotation
   
   - ğŸš¨ Your `@KafkaListener` method throws an exception if processing fails. \
   - ğŸ”„ Spring Kafkaâ€™s `@RetryableTopic` infrastructure catches the exception and:

      - ğŸ” Retries **in-memory first** (retry attempts happen inside the listener container)
     
      - ğŸ“¤ If all in-memory retries fail, it **sends the message to the retry topic automatically** (like `order-topic-retry-0`)
     
      - ğŸ‘‚ Spring Kafka automatically creates a listener on the retry topic and processes the message again. 
      - âŒ If the retry topic processing fails again, Spring Kafka sends the message to the **dead-letter topic automatically**.

- ğŸš« **No** `kafkaTemplate.send()` to retry topic or DLT
- ğŸš« **No** manual offset commits or error handlers for retries
- ğŸ›‘ Just **throw exceptions for failures** â€” Spring Kafka handles routing and retries!

```bash

@RetryableTopic(
    attempts = 4, // total attempts: 1 main + 3 retries (2 in-memory + 1 async)
    backoff = @Backoff(delay = 5000), // retry delay = 5 seconds
    retryTopicSuffix = "-retry",
    dltTopicSuffix = "-dlt",
    autoCreateTopics = false
)
  
/** If you want to use your own retry topic names, disable auto-creation and create the topics yourself
Then you must create:

order-topic-retry
order-topic-dlt

You can use Kafka CLI or Spring Kafka NewTopic bean.

if not enable  auto-create-topic as true
*/
```

---

### âš ï¸ Limitations of `@RetryableTopic` for Exactly-Once Semantics (EOS):

- ğŸ”„ It uses **automatic offset commits**, which align with **at-least-once** delivery.
- âŒ It **doesnâ€™t handle Kafka transactions** or atomic commits of offsets and side effects.
- ğŸ› ï¸ You **lose fine-grained control** needed for EOS guarantees.



---

# ğŸ§  Idempotent Consumer in Kafka

An **Idempotent Consumer** is a system that can `receive the same message multiple times` but `only processes it once`.

> ğŸ§® In math:  
> `f(x) = f(f(x))` â†’ Applying it once or multiple times = same result.

## ğŸ” When does Kafka send the same message again?

Kafka can **redeliver a message** when:

- ğŸ’¥ The consumer crashes
- ğŸ”„ There's a rebalance
- âŒ Offset is not committed
- ğŸŒ Network issues


## ğŸ•’ When to use Idempotent Consumers?

Use them whenever:

- ğŸ”‚ Messages `might be delivered more than once` (which happens often in real-world systems)
- âš ï¸ You `canâ€™t afford to do the same operation multiple times`  
  (like charging money, shipping products, sending emails, etc.)


## ğŸ†” ID Tracking Mechanism

Yes, the consumer needs to `create` and `manage` the `ID tracking` mechanism.

Because `only the consumer knows` which messages it has received and processed â€” so itâ€™s the **consumerâ€™s job** to:

- ğŸ•µï¸â€â™‚ï¸ Track processed message IDs (e.g., `orderId`)
- ğŸ—‚ Store them in a `reliable place` (like a database, cache, or external system)
- âœ… Check before processing: If already processed, **skip it**

## ğŸ§‘â€ğŸ’» Who creates this?

ğŸ‘‰ **You (as the developer)** need to implement it.

- âŒ Spring Kafka does `not provide automatic idempotency` at the consumer level.
- âœ… Kafka **allows duplicate delivery** (especially in case of retries, crashes, network issues,rebalances, etc.)
- ğŸ”§ So, **your code in the consumer** must manage message **deduplication**.


## ğŸ›  Options for Storing Processed IDs

| Option           | Description                              | âœ… Pros                   | âš ï¸ Cons                        |
|------------------|------------------------------------------|---------------------------|--------------------------------|
| ğŸ§  In-memory Set  | Simple Java `Set` or `Map`               | Fast, easy                | Lost on restart; not for prod |
| ğŸ—ƒ Database        | Store `orderId` in a SQL/NoSQL table     | Persistent, reliable      | Slower, adds DB dependency    |
| ğŸš€ Redis          | Fast, distributed key-value store        | Fast + persistent         | Requires Redis server         |
| ğŸ“¦ Kafka log compaction | Use a compacted topic to track IDs | Native to Kafka           | More complex, advanced use    |


## ğŸ” Idempotent Consumer Flow (with manual offset commit)

1. ğŸ“¥ Kafka delivers a message to the consumer
2. ğŸ” Consumer extracts unique ID (like `orderId`)
3. ğŸ§¾ Consumer checks a DB or Redis if `orderId` is already processed
4. âœ… If **not processed**:
  - ğŸ”§ Process the message
  - ğŸ’¾ Save `orderId` to DB
  - ğŸ“ Commit the offset (manually if `auto-commit = false`)
5. ğŸš« If **already processed**:
  - â­ Skip the message
  - ğŸ“ Still commit offset (to avoid re-delivery)

```bash

Kafka -> Spring Consumer -> Extract orderId (Tracking ID/Message ID)
                        |
                        v
               Is orderId already in DB?
                    /         \
                Yes            No
                /               \
           Skip it         Process it
                              |
                              v
                      Save orderId to DB

```

---

