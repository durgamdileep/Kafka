# ðŸ§© Topic

A **Topic** is a `logical grouping` of messages/events.

- ðŸ“¦ It `does not store data directly` â€” it `just organizes messages under a name`. 
- ðŸ”„ Topics allow **multiple producers** to send and **multiple consumers** to read messages using the topic name. 
- âš ï¸ A topic should contain `only one kind of logically related message` â€” messages from different domains (like orders, payments, inventory) should be `placed in separate topics`.

### âœ… Example:

ðŸ›’ In an e-commerce system:

- Use `order_topic` for all order-related messages (like:
    - ðŸ“¬ order placed,
    - ðŸšš order shipped)

âŒ Do not send payment or inventory messages into `order_topic`.  
âœ… Those should go into:
- `payment_topic` ðŸ’³ or
- `inventory_topic` ðŸ“¦.

---

# ðŸ“¦ Partition

A Partition is the `actual storage unit` of Kafka where messages are written.

- ðŸ“‚ Each `Topic` is divided into `one or more Partitions`. 
- 1ï¸âƒ£ By default, a topic has `one partition`. 
- ðŸ“ Partitions are `append-only logs` â€” messages are written in sequence.
- â³ `Ordering` is guaranteed only within a single partition, `not across partitions`. 
- #ï¸âƒ£ Each partition is identified by a `partition number (0, 1, 2, â€¦)` under a topic.
- âš™ï¸ Partitions allow `parallelism` â€” multiple partitions can be read/written in parallel by different consumers/producers. ðŸ”€
   - Broker-Level Parallelism
     - Kafka brokers store partitions on disk. 
     - `Multiple partitions` = `more disk I/O threads` `working in parallel` (especially with multiple disks or SSDs). 
     - Also, partitions can be spread across multiple brokers (horizontal scaling).
- ðŸ” `Each partition` can have `replicas` for `fault tolerance`, though `only one replica` is the `leader` at a time. ðŸ›¡ï¸
- ðŸ”¢ `Each message` within a partition has a `unique sequential ID` called an `offset`, which is important for `consumer tracking`. ðŸ“


## ðŸ—‚ï¸ Kafka Partition is a Log, Not a Queue

- ðŸ“¥ In a **Queue**, once a message is read, it is removed. You cannot re-read it.

- ðŸ“œ In Kafka, a **Partition** acts like a log:
  - ðŸ“ Messages are stored in sequential order.
  - ðŸ•’ Even after reading, messages are not deleted immediately.
  - ðŸ”„ Messages can be read multiple times based on log retention settings.

âœ… Thatâ€™s why **Kafka Partition is a Log, not a Queue**.

---

# ðŸ“¬ What Does a Kafka Message Contain?

- ðŸ”‘ **key**: Used for partitioning logic. Not necessarily unique.
- ðŸ“¦ **value**: Actual data (can be JSON, Avro, Protobuf, etc.).
- â° **timestamp**: When the message was written to the partition.
- ðŸ—‚ï¸ **partition**: Partition number where the message is stored.
- ðŸ”¢ **offset**: Position of the message in the partition. Unique within a partition.
- ðŸ·ï¸ **headers**: Optional key-value pairs for metadata (e.g., tracing, filtering).
- Example Message:
``` 
    {
      "key": "user-123",
      "value": {
        "event": "user_signup",
        "userId": "123",
        "timestamp": "2025-10-12T15:45:00Z",
        "details": {
          "email": "user@example.com",
          "plan": "premium"
        }
      },
      "timestamp": 1697115900000,
      "partition": 3,
      "offset": 4567,
      "headers": {
        "traceId": "abc123xyz",
        "source": "web_app"
      }
    }
    // key: "user-123" â€” used for partitioning, e.g., all events for this user go to the same partition.
    // value: The actual payload of the message.
    // timestamp: Unix epoch milliseconds when the message was written.
    // partition: Partition number (3 in this case).
    // offset: Unique offset within the partition.
    // headers: Metadata like tracing info or source system.

```
---

## ðŸ”‘ Message/Event Without Key (Case 1)

- âŒ If we send a message **without a key**:
- ðŸ”„ Kafka uses `round-robin` to distribute messages evenly across partitions.

- ðŸ“Š **Example**:
  - We have `3 partitions` for a topic.
  - Sending `3 messages` for `sensor_id: 42` without a key â†’ each message may go to a **different partition**.

- âš ï¸ **Problem**:
  - Messages from the **same sensor** are stored in **different partitions**.
  - Kafka guarantees message **ordering only within a single partition**, but **not across partitions**.

- âš¡ **Consequence**:
  - We **lose the order** of messages when reading them, because messages from the **same source are scattered** across partitions.


## ðŸ”‘ Message/Event With Key (Case 2)

- âœ… If we send a message **with a key**:

- ðŸ§® Kafka hashes the key and assigns it to a partition using:  
  `partition = hash(key) % number_of_partitions`

- ðŸŽ¯ All messages with the **same key** go to the **same partition** then Order of insertion and reading the message will be same.

  - ðŸ“Š Example:  
    10 messages with various sensor IDs as keys:
    ```
    Key: 42 â†’ { sensor_id: 42, ... }
    Key: 83 â†’ { sensor_id: 83, ... }
    Key: 42 â†’ { sensor_id: 42, ... }
    Key: 37 â†’ { sensor_id: 37, ... }
    Key: 55 â†’ { sensor_id: 55, ... }
    Key: 42 â†’ { sensor_id: 42, ... }
    Key: 77 â†’ { sensor_id: 77, ... }
    Key: 100 â†’ { sensor_id: 100, ... }
    Key: 88 â†’ { sensor_id: 88, ... }
    Key: 99 â†’ { sensor_id: 99, ... }
    Key: 100 â†’ { sensor_id: 100, ... }
    ```

- ðŸ”¢ After hashing:
    ```
    Partition 1 â†’ keys: 100, 99, 100, 88
    Partition 2 â†’ keys: 77, 55
    Partition 3 â†’ keys: 42, 42, 83, 42
    ```

- ðŸ”’ Messages with the **same key** (e.g., 42, 100) go to the **same partition**, so:
- ðŸ§­ **Order is preserved** for messages with the same key.
- âš™ï¸ Kafka guarantees **message order within a partition**.

---

# ðŸ–¥ï¸ Apache Kafka Broker

A broker in Apache Kafka is `a server` that is part of a Kafka cluster. It is responsible for:

- ðŸšš **Receiving messages** from producers,
- ðŸ—„ï¸ **Storing** those `messages on disk`,
- ðŸ“¦ **Serving messages**  to consumers when they request them.

Each broker handles part of the data (topics/partitions), and Kafka can have `many brokers working together in a cluster` to provide `scalability` and `fault tolerance`.

---

# ðŸ§© Partition Distribution

ðŸ“¦ Suppose you create a topic `order_topic` with **5 partitions**.

Kafka assigns each partition to a broker based on:

- ðŸ–¥ï¸ **Available brokers** in the cluster
- ðŸ“‘ **Replication factor** (how many copies of each partition to create)
- ðŸ“ˆ Kafkaâ€™s **partition assignment strategy**:
  - ðŸ” Typically **round-robin**
  - ðŸ§  Or **rack-aware balancing** if enabled


- `Partition and replica distribution` decisions are always made by the `Kafka controller` component (broker).
- In `KRaft` mode, the `Kafka controller` is part of the `KRaft controller`.
   - Kafka Controller = component responsible for partition/replica management. 
   - KRaft = new metadata mode where this controller is built into Kafka, replacing ZooKeeper.


### ðŸ§ If there is only **1 broker**:

   - âž¡ï¸ All partitions of the topic go to that **one broker**.

### ðŸ‘¥ If there are **multiple brokers**:

   - ðŸ”„ Kafka spreads (distributes) partitions across brokers.
   - 1ï¸âƒ£ First, it puts **1 partition on each broker**.
   - ðŸ” If partitions are still left, it goes back to the **first broker** and continues.
   - ðŸ”š This goes on until **all partitions are assigned**.


### ðŸ“Š If `number of brokers > number of partitions`:
    
   - ðŸš« Only **some brokers** will get partitions.
   - ðŸ™… Other brokers will **not have any partition** of that topic.

### âš–ï¸ Partitions are **not always evenly divided**:

   - ðŸ“‰ All brokers may **not get the same number** of partitions.
   - ðŸ“ˆ Some brokers may have **more**, others **less**, depending on total partition count.

### ðŸ› ï¸ Topic Creation:

   - ðŸ§  When you **create a topic**, Kafka normally decides how to spread partitions to brokers **automatically**.
   - ðŸ‘¤ You (the user) can **manually assign** which partition goes to which broker.
   - ðŸ‘‰ If you do this **manual assignment**, then **Kafka does not use its normal logic** (the round-robin distribution).

## ðŸ”„ Partition Assignment Strategy:

- âœ… **If you let Kafka decide** â†’ it uses the **round-robin** strategy.
- âœ³ï¸ **If you assign manually** â†’ your **custom setup** is used.

## ðŸ“ Partition  Placement: Who Decides & Spring Boot Ease

| Case                   | Who Decides Partition Placement?           | Easy to Do in Spring Boot?              |
|------------------------|---------------------------------------------|----------------------------------------|
| âš™ï¸ Auto Assignment       | Kafka (default behavior)                     | âœ… Yes                                 |
| ðŸ› ï¸ Manual Assignment (Advanced) | You (custom broker/partition mapping)         | âš ï¸ Needs Kafka AdminClient              |


---

# ðŸ§­ Kafka Partition Leadership & Replication

## ðŸ‘‘ Each Partition Has:

- ðŸŸ¢ **One Leader**
  - ðŸ§  Responsible for `handling all client requests` (both reads and writes)
  - ðŸ“¤ Producers send data to the leader
  - ðŸ“¥ Consumers fetch data from the leader

- ðŸ”„ **Zero or More Followers**
  - ðŸ“š Followers `replicate data` from the leader
  - ðŸš« `Do not serve client requests`
  - ðŸ” Keep in sync with the leader (used for failover)
  - ðŸ”„ Only replicate data from the leader 
  - ðŸ§ Do not handle any client operations
       - âŒ No writes from producers 
       - âŒ No reads from consumers
  - ðŸ§¯ Serve as backup in case the leader fails 
  - ðŸª„ Can be promoted to leader if the current leader goes down

## ðŸš¨ Replicas Read/Write Behavior

### âš™ï¸ Default Behavior
- âŒ By default, `replicas do not handle read/write operations` from clients (producers and consumers).
- ðŸ”‘ The leader handles all those requests.

### ðŸš€ Performance Optimization
- âš™ï¸ Some systems allow `configuring replicas` to `serve read-only` operations to `clients`.
- ðŸ’¡ This `reduces load on the leader` and `improves latency`.
- ðŸ–¥ï¸ So replicas can be used for **read operations only**.

### Summary:
| Mode                  | Read from Replicas      | Write from Replicas    |
|-----------------------|------------------------|-----------------------|
| **Default**            | âŒ No                  | âŒ No                 |
| **With Configuration** | âœ… Yes                 | âŒ No                 |


---

## ðŸ”„ Kafka Replication Factor 

When a topic is created with a **replication factor > 1**  
Kafka assigns additional replicas for **high availability**.

Kafka tries to:

- âŒ **Avoid placing the leader and its replicas on the same broker**
- âš–ï¸ **Balance replicas evenly across all brokers**



### 1ï¸âƒ£ Replication Factor = 1

- ðŸŸ¢ Only **1 copy** of each partition (the **leader**)
- âš ï¸ **No backup**
- ðŸ’¥ If that broker goes down, **data is lost**

### 2ï¸âƒ£ Replication Factor > 1

Each partition will have:

- ðŸ‘‘ **1 Leader**
- ðŸ” **(replication factor - 1) Replicas** (backups on other brokers)

---

## ðŸ¤” How Does Kafka Distribute Replicas When Replication Factor > 1?

- ðŸ¤– Kafka **automatically** gets the list of brokers **except the leader** and distributes the replicas among them.
- âš–ï¸ Tries to **distribute replicas evenly** across brokers, similar to how partitions are distributed.
- ðŸš« Tries to **avoid placing a replica on the same broker as the leader**.

---

## âš ï¸ Notes

- âŒ Kafka does **not guarantee perfectly equal** replica distribution.
- ðŸ› ï¸ You can **customize** replica placement using a **manual `ReplicaAssignment`** if needed.



---

## ðŸ§ª Example:

ðŸŽ¯ Topic: `order_topic`  
ðŸ“‚ Partitions: 3  
ðŸ” Replication Factor: 2  
ðŸ–¥ï¸ Brokers: 3

| Partition | Leader Broker | Replica Brokers |
|-----------|----------------|------------------|
| P0        | Broker 1       | Broker 2         |
| P1        | Broker 2       | Broker 3         |
| P2        | Broker 3       | Broker 1         |

âœ… Clients talk only to leaders.  
ðŸ” Followers passively sync and take over if a leader fails.

---

## ðŸš¨ Important Notes:

- ðŸ›‘ If the **leader fails**, Kafka controller promotes a **follower to leader**.
- âœ… Kafka ensures that at least one replica is **in-sync (ISR)** to maintain consistency.

---

## âš ï¸ Important Constraints

- ðŸ§  `Replication Factor` â‰¤ `Number of Brokers` 
   - Kafka cannot place more replicas than the number of brokers.
- ðŸš« â†’ `No broker` can `host multiple replicas` of the `same partition`. 
   - Kafka `does not place more than one replica` of the `same partition` on the `same broker`.

### ðŸ“Œ Example

ðŸ“¦ **Topic**: `order_topic`  
ðŸ§© **Partitions**: `3`  
ðŸ–¥ï¸ **Brokers**: `3`  
ðŸ“‘ **Replication Factor**: `5`

âŒ **Invalid Scenario**: RF = 5, Brokers = 3

âš ï¸ Kafka throws an error like:

```bash

InvalidReplicationFactorException: Replication factor: 5 larger than available brokers: 3
```

---

# ðŸ¤” What is a Kafka Consumer?

- ðŸ“© A consumer `reads messages from Kafka topics`.
- â¬‡ï¸ It pulls data (Kafka doesnâ€™t push). 
- ðŸ‘¥âš–ï¸ `Consumers are grouped` into `consumer groups` to `share the load`.

---

### ðŸ§ª Example Without Consumer Group

ðŸ“Œ Let's say you have a Kafka topic called orders, and it contains these 4 messages:  
ðŸ“¦ Message 1: Order#101  
ðŸ“¦ Message 2: Order#102  
ðŸ“¦ Message 3: Order#103  
ðŸ“¦ Message 4: Order#104

ðŸ‘¥ You have 2 consumers (Consumer A and Consumer B), but they are `not part of a consumer group` (i.e., each has a unique group ID or no group at all).

â“ What happens?

- ðŸ‘¤ Consumer A reads: Order#101, Order#102, Order#103, Order#104 
- ðŸ‘¤ Consumer B reads: Order#101, Order#102, Order#103, Order#104


ðŸ” So, `instead of splitting the work`, `both consumers process` the `same messages`.

---

### ðŸ” Problem Without Consumer Group

- âŒ `Duplicate processing`: Every consumer gets the full data set. 
- âŒ `No load sharing`: No performance benefit from adding more consumers. 
- âŒ `Wasted resources`: Youâ€™re using multiple consumers without gaining parallelism.

---

### ðŸ§  What is a Consumer Group?

- ðŸ‘¥ A `consumer group` is a `way to split the work` of `reading messages` from a Kafka topic `among multiple consumers`.
- ðŸ†” All consumers in a group `share a group ID`. 
- âœ… Kafka makes sure that `each message is read by only one consumer` `within a group`.

ðŸ‘‰ Think of it like a team of workers sharing a task.

```bash
   if Consumer C1 reads Message 1, then no other consumer in the same group will read Message 1.
```

---

### ðŸ‘€ When to Use a Consumer Group?

- âš¡ Use a consumer group when you want to `process messages in parallel` to `make your system faster` and `more scalable`. 
- âœ… Use consumer groups to `distribute the load` across `multiple consumers`.

---

### ðŸ§ª Example

ðŸ“Œ You have a Kafka topic called `order-topic`,  
ðŸ“Œ Assume the topic has 2 partitions, and 4 messages across them:

ðŸ“ Topic: order-topic  
ðŸ“‚ Partition 0: Order#101, Order#102  
ðŸ“‚ Partition 1: Order#103, Order#104

ðŸ‘¤ Kafka assigns partitions like this:

- ðŸ‘¤ Consumer A reads: Order#101, Order#102 (from Partition 0)
- ðŸ‘¤ Consumer B reads: Order#103, Order#104 (from Partition 1)

âœ… `Each message` is `read only once` within the `same group`, and the work is shared between consumers â€” thanks to `consumer assignment`.

---

### ðŸ”„ What happens?

ðŸ§‘â€ðŸ’» `Kafka Controller` `divides the partitions` to `consumers in a consumer group`
- ðŸ“Š Kafka controller (via the group coordinator) assigns partitions to consumers in the group â€” this is called `consumer assignment`.
   - âš™ï¸ By default, it uses built-in strategies like `Range`, `RoundRobin`, or `CooperativeSticky`. 
   - ðŸ› ï¸ If needed, you can use a `custom consumer assignment strategy`.

---

## âš™ï¸ How Message Consumption Works

### âž¡ï¸ Within a single partition:
- âž¡ï¸ Messages are consumed one at a time in order (sequentially).

### âš¡ Across different partitions:
- âš¡ `A consumer` can `consume messages` in `parallel from multiple partitions assigned to it`.

ðŸ“ Topic : ðŸ“¦ `order-topic`  
ðŸ‘¥ Consumer Group : ðŸ‘¥ `G1`  
ðŸ—‚ï¸ Partition Assignment in Consumer Group
   - Consumer C1 â†’ `Partitions P0` and `P1`
   - Consumer C2 â†’ `Partition P2`

### ðŸ“Œ Example with Consumer C1

Since C1 is assigned partitions P0 and P1, it can:

- âœ… Process `a message from P0` and `a message from P1` at the same time.
- âŒ But it `cannot process multiple messages from P0 simultaneously`, `nor multiple messages from P1 simultaneously`.

---

# ðŸ“¦ Kafka Consumer Group Partition Assignment

## ðŸ”„ Partition assigning to consumer group (Consumer Assignment)

### âž¤ Rules:
In a consumer group:

- âœ… Each partition is assigned to only one consumer.
- ðŸ” A consumer can be assigned multiple partitions.
- âš–ï¸ Kafka by default uses `Range`, `RoundRobin`, or `CooperativeSticky` strategies for distribution.
     - ðŸ“¦ `RangeAssignor` (**default**): Contiguous blocks of partitions 
     - ðŸ”„ `RoundRobinAssignor`: Evenly round-robins across consumers 
     - ðŸ§· `StickyAssignor`: Tries to avoid moving partitions during rebalances (preferred for stability)
     - ðŸ¤ `CooperativeStickyAssignor`: Allows smooth rebalancing without stopping all consumers

- ðŸ› ï¸ If needed, you can use a `custom consumer assignment strategy`.

---

### ðŸ’¡ For Suppose:

In Kafka, consumer groups are used to distribute partitions among consumers. However:

- ðŸ”¢ If we have `no. of consumers in a group > no. of partitions`
   - then some consumers will not have any partition, they will be `idle consumers` (i.e., not assigned any partition).
- âž— If we have `no. of consumers < no. of partitions`
   - then each consumer will be assigned `multiple partitions`.
- ðŸŸ° If we have `no. of consumers == no. of partitions`
   - then each consumer will have `one partition`.

```bash

Topic: order-topic
Partitions: 3
Consumers: C1, C2
Group: group-A
   - C1 â†’ P0, P2  
   - C2 â†’ P1
```

### ðŸ“ Summary:

- âœ… Kafka guarantees that `each partition is assigned to only one consumer in a group`.
- âš ï¸ But it does `not guarantee` that `each consumer will get a partition`.

---

## ðŸ“š Multiple Consumers Reading and Assigning Same Partition

- âŒ Not allowed within the same consumer group.
- âœ… Allowed if consumers are in different consumer groups.

```bash

Invalid
Group-A 
  - C1 â†’ P0 
  - C2 â†’ P0

Valid
Group-A â†’ C1 â†’ P0  
Group-B â†’ C2 â†’ P0
```

---

## ðŸ” Can a Consumer Read from Multiple Partitions?

ðŸ§‘â€ðŸ’» `A single consumer` can be assigned `multiple partitions` and pull messages from each.
```bash

Group: group-A
   - C1 â†’ P0, P2  
   - C2 â†’ P1
```
---

## â“ Can the same messages be consumed by different consumer groups?

```bash

Topic: order-topic
Partitions: 3
Consumers: C1, C2
Group: 
  group-A(G1)
     - C1 â†’ P0, P2  
     - C2 â†’ P1
  group-B(G2)
     - C1 â†’ P0,P2
     - C2 â†’ P1
```

### ðŸ‘¥ In Consumer Group G1:

- ðŸ‘¤ Consumer C1 reads Partition P0 â†’ Message M1
- ðŸ‘¤ Consumer C1 also reads Partition P2 â†’ Message M2  
  âœ… C1 can consume messages from both partitions at the same time **within G1**.

### ðŸ‘¥ In Consumer Group G2:

- ðŸ‘¤ Consumer C1 (a different consumer instance in G2) reads Partition P0 â†’ Message M1
- ðŸ‘¤ Consumer C1 reads Partition P2 â†’ Message M2  
  âœ… Since G2 is a **different group**, it can consume the **same partitions and messages** independently from G1.

---



