# ğŸ§­ Kafka Offset Management

- ğŸ“˜ In Kafka, offset is the `position (unique number) of a message` in `a partition` â€” like a bookmark that `tells the consumer where it left off reading`. 
- ğŸ” `Consumers must commit the offset` of messages `after they are successfully processed` to `avoid reading the same message again` `after a crash or restart.`

---

## ğŸ“¦ Example:

Letâ€™s say a Kafka topic: `Order-topic` and Partition: `P0` has the following messages:

| Offset | Message       |
|--------|---------------|
| 1      | "Order #1001" |
| 2      | "Order #1002" |
| 3      | "Order #1003" |

---

## 1. 1ï¸âƒ£ Automatic Offset Commit (At-Most-Once Delivery) -> message is delivered 0 or 1 time

- âš¡ At-most-once delivery: `Message is delivered 0 or 1 time` â€” `no duplicates`, but `message loss is possible`.
- âš¡  `Fast delivery`
- âŒ `may lose messages`
- ğŸ“˜ Kafka automatically `saves the offset at regular intervals` (default: every 5 seconds), whether `the message is processed or not`.

### âš™ï¸ How It Works

- âš™ï¸ **Property**: `enable.auto.commit = true` (default is true)
- **What happens:**
  - ğŸ”„ Kafka's consumer automatically commits the latest offset returned by the `poll` at regular intervals (defined by `auto.commit.interval.ms`, default is 5000ms or 5 seconds).
  - ğŸš« `No manual commit needed`.
  - ğŸ§  `Offsets are committed` `asynchronously` in the background.

### ğŸ“¦ Example:

You read message at offset 3:  
ğŸŸ¢ `"Order #1003"`

- Kafka waits 5 seconds and automatically commits offset 3.
- If your `app crashes after reading` `but before 5 seconds`, on restart, Kafka sends offset 3 again.


### âš™ï¸ How It Works Internally:

- ğŸ“¥ Consumer reads message
- ğŸ“Š Kafka tracks the offset
- â±ï¸ Every 5 seconds (default), Kafka saves the current offset to a special Kafka topic called `__consumer_offsets`
- ğŸ” On restart, Kafka starts from `last committed offset + 1`


### âŒ Problems:

- âš ï¸ If the app crashes before Kafka auto-commits, the message is read again (`possible duplicates`)
- âŒ You can't control when the offset is saved
- ğŸ” Risk of processing the same message twice


### âš ï¸ Caveats

- â³ You donâ€™t control exactly when the commit happens.
- ğŸ’¥ If the consumer crashes after polling but before processing, the offset may be committed before the message is handled â†’ `message loss` (lost messages if offsets are committed too early)


### ğŸ•°ï¸ When to Use:

- âš¡ When message `processing is fast` and `not critical`
- ğŸ”„ You `donâ€™t mind occasional duplicate processing`
- ğŸª„ Use for `logging`, `analytics`, or `simple data pipelines`


### âœ… Pros:

- ğŸ‘ Very easy to use
- ğŸ’¡ No manual code needed to commit
- âš™ï¸ `Good for simple`, `fast-processing apps`

### âŒ Cons:

- ğŸš« `Not reliable` for `critical data`
- ğŸ” Risk of duplicates
- âŒ No control over when offset is saved

---

## 2. 2ï¸âƒ£ Manual Offset Commit(At-Least-Once delivery) -> Message is delivered 1 or more times

- ğŸ” At-least-once delivery: `Message is delivered 1 or more times` â€” `no loss`, but `duplicates are possible`.
- âœ… `Reliable delivery`, may have duplicates

- ğŸ“˜ You (the developer) manually tell Kafka:
> â€œIâ€™ve finished processing this message â€” now save the offset.â€

- when the consumer commits offsets â€” typically **after successful processing** of messages.
- This helps ensure `at-least-once` delivery semantics, `reducing the risk of message loss` due to premature offset commits.


### ğŸ”§ Manual Offset Commit â€“ How It Works

**Step-by-step flow:**

1. ğŸ“¥ Consumer polls messages from Kafka
2. ğŸ› ï¸ Processes the messages (e.g., saves them to DB)
3. âœ… Commits the offsets manually (either synchronously or asynchronously)

ğŸ’¡ If the consumer fails before commit, the messages will be re-consumed.


### âœ… Enabling Manual Commit

- âš™ï¸ Set `enable.auto.commit = false`


### ğŸ§  Commit Types

#### ğŸ”¹ `commitSync()`

- â›” `Blocking`
- ğŸ” Waits for Kafka to confirm the offset is committed.
     -  If commit fails, the consumer knows immediately and can:
         - Log the error. 
         - Retry the commit.(retry logic can be automatic in your code because you get immediate failure notification).
- âœ… Ensures the offset is safely stored.
     - You can handle failures more reliably with commitSync() because you get a clear success/failure signal.
- ğŸŒ `Safer but slower`.

   ##### ğŸ“¥ What happens:
     - ğŸ“¤ Consumer sends a commit request to Kafka with the latest offset. 
     - ğŸ“¦ `Kafka acknowledges the commit only` `when the offset is stored in its internal offsets topic` (`__consumer_offsets`). 
     - â³ `Consumer waits until Kafka replies` with **"OK, I saved it"**. 
     - âœ… Once that happens, everything is safe.
   - ğŸ§± If the `consumer crashes` `after Kafka stores the offset`, the `offset` is `safe inside Kafka`. 
   - âš ï¸ If the `consumer didnâ€™t receive the ACK` from `the broker`, it `may be unaware that the offset was successfully committed`, However, `since the offset is safely stored inside Kafkaâ€™s internal topic` (__consumer_offsets), this `does not cause re-fetching or duplicates`.
   - ğŸ” When the consumer restarts, it will `continue` from the `last committed offset in __consumer_offsets.` 
   - ğŸ“¡ `Kafka itself` `doesnâ€™t â€œresendâ€ the message` â€” the `consumer re-requests` it because it believes the previous commit failed.
   - ğŸ’¡ Final verdict: `Whether the consumer receives the ACK or not`, `if the offset is stored inside __consumer_offsets`, it `does not lead to re-fetching or duplicates`.

#### ğŸ”¹ `commitAsync()`

- âœ… `Non-blocking`
- ğŸš€ Fire-and-forget: wonâ€™t wait for confirmation.
- âš¡  `Faster`, but `no guarantee commit` `was successful`.
   - No retry logic:
      - If the `commit fails` (e.g., network error, broker issue), the `consumer won't know` , unless you provide a callback â€” `it doesnâ€™t wait or retry`. 
      - With `sync commit`, `you can catch the failure and retry`.
- ğŸ§© You can also provide a `callback` to handle failures and implement your own retry logic if needed.

  ##### âš™ï¸ What happens:
     - ğŸ”„ Consumer `sends the offset to Kafka asynchronously` (in the background). 
     - â© `Consumer does NOT wait` for `Kafka to confirm`. 
     - ğŸ“¦ Kafka tries to store the offset in its internal topic, but:
          - âœ… If successful: Offset is saved. 
          - âŒ If failed: Offset might not be saved.
     - ğŸ’¥ If the consumer crashes immediately after, Kafka might not have stored the offset yet. 
     - â¡ï¸ **Result:** Kafka thinks the message was never processed, and sends it again when the consumer restarts â†’ ğŸ” duplicate processing.


### âš ï¸ Key Considerations

| ğŸ§© Aspect        | ğŸ” `commitSync()` | ğŸš€ `commitAsync()` |
|------------------|-------------------|---------------------|
| â±ï¸ Blocking?     | âœ… Yes            | âŒ No              |
| ğŸ›¡ï¸ Reliable?     | ğŸ”’ High           | âš ï¸ Medium          |
| âš™ï¸ Performance   | ğŸ¢ Slower         | âš¡ Faster           |
| ğŸ§ª Use Case      | ğŸ§¾ Critical processing | ğŸ“Š High-throughput scenarios |


## Kafka Offset Commit Comparison

### commitAsync()

- âš ï¸ **No automatic retry:**
    - ğŸ“¨ When you call `commitAsync()`, the Kafka client sends the commit request **asynchronously** and immediately returns.
    - âŒ If the commit fails (network error, broker down, etc.), the client **does not retry automatically**.
    - â„¹ï¸ Kafka might succeed or fail in storing the offset, but the consumer has no guaranteed way to know unless a callback is used.

- ğŸ”” **Callback option:**
    - ğŸ“£ You can provide a callback to `commitAsync()` to get notified if the commit **succeeded or failed**.
    - ğŸ” If it fails, you can **manually retry** inside the callback.

- â© **Non-blocking:**
    - ğŸƒ The consumer continues processing messages **regardless of commit success**.
    - âš¡ This makes `commitAsync()` faster but **less safe for exactly-once guarantees** compared to `commitSync()`.

### commitSync()

- ğŸ”„ **Automatic retry:**
    - ğŸ” `commitSync()` automatically retries internally for any retriable exceptions (like network issues, leader not available, or temporary broker failures).

- â³ **Blocking behavior:**
    - ğŸ›‘ It blocks the caller until either:
        - âœ… The offset is **successfully committed**, or
        - âŒ A **non-retriable exception** occurs.

- ğŸ›  **No extra retry logic needed:**
    - ğŸ’¡ You donâ€™t need to implement your own retry logic â€” the Kafka client handles it automatically.

- âš ï¸ **Exception handling:**
    - ğŸ“ The only thing you need is a `try-catch` around `commitSync()` to catch final exceptions if all internal retries fail.

- ğŸ” **Guaranteed commit retry:**
    - ğŸ”’ Automatic retry continues internally **until the commit succeeds or a fatal error happens**.



### ğŸ” Why Manual Commit?

- âœ… To `ensure message processing` before committing.
- ğŸ” To `implement retry mechanisms` or `transactional processing`.
- ğŸ§· To `prevent data loss` in case of consumer failure.

### ğŸ“¦ Example:

You read message at offset 3:  
ğŸŸ¢ `"Order #1003"`  
You finish saving it to DB â†’ then call:  
ğŸ”§ `commitSync()` or `commitAsync()`

âœ… Now Kafka saves offset 3.

- ğŸ’¥ If your app crashes before committing, Kafka will re-send offset 3. 
- ğŸ›¡ï¸ You can make sure you donâ€™t commit until processing is successful.

### âš™ï¸ How It Works Internally:

- ğŸ“¥ Consumer reads message
- ğŸ› ï¸ You process it (e.g., save to DB)
- ğŸ”§ You manually call `commitSync()` or `commitAsync()`
- ğŸ§  Kafka stores the offset in `__consumer_offsets`
- ğŸ” On restart, Kafka resumes from **last committed offset + 1**

### âŒ Problems:

- ğŸ‘¨â€ğŸ’» You must write extra code
- ğŸ•³ï¸ If `you forget to commit` â†’ Kafka will `keep resending old messages`
- ğŸ’£ `Improper handling can lead` to `duplicate processing`, `not message loss`

### ğŸ•°ï¸ When to Use:

- ğŸ›¡ï¸ For `critical applications` (e.g., orders, payments)
- For applications where `losing messages is unacceptable`
- ğŸ§  When you can handle duplicates safely (idempotent operations)
- âœ… For `critical data` where `full control over processing` is required


### âœ… Pros:

- ğŸ” Safer and more reliable than auto-commit without processing guarantees
- ğŸ’¼ Good for **critical business data**
- ğŸ’¼ `Guarantees no message loss` if offset is committed after processing 
- âš–ï¸ Allows fine-grained control over processing logic


### âŒ Cons:

- ğŸ‘¨â€ğŸ’» More complex
- ğŸ§¾ Requires manual code to commit
- â— Mistakes can lead to `duplicate processing (reprocessing)`, but `not message loss`

---

Rebalancing

---
# ğŸš€ Kafka Consumer Lag

In Kafka, a consumer reads messages from a topic (think of a topic as a stream of messages).

**ğŸ“ Producer:** Sends messages to Kafka.  
**ğŸ‘¤ Consumer:** Reads messages from Kafka.

## âš¡ Consumer Lag

The` number of messages` `in the topic` that `the consumer has not read yet`.

- âœ… If lag is 0, the consumer is caught up.
- â³ If lag is 100, the consumer is 100 messages behind.

### ğŸ“ Simple Example with Data

Imagine a topic `orders`:

| Offset | Message |
|--------|---------|
| 0      | order1  |
| 1      | order2  |
| 2      | order3  |
| 3      | order4  |
| 4      | order5  |

Producer adds two more messages:

| Offset | Message |
|--------|---------|
| 5      | order6  |
| 6      | order7  |

Now, the consumer has not read offsets `3, 4, 5, 6`.

**Consumer lag = 4 messages.**

### Consumer lag can be calculated as:
`Consumer Lag = Latest Offset âˆ’ Consumerâ€™s Current Offset`

``` bash

From the example:  
   - Latest offset = 6  
   - Consumer offset = 2
   - Lag = 6 âˆ’ 2 = 4
```



## âš ï¸ The Problem: Consumer Lag

The `problem arises` when `your consumer is slower` than `the producer`, meaning it cannot keep up with the incoming messages.

Think of it like this:

- ğŸ“¦ Messages are coming down a conveyor belt (Kafka).
- ğŸ‘· The consumer is a worker picking up packages. 
- If the worker is slow, packages pile up â€” that pile is your `lag`.

### âŒ Why Itâ€™s a Problem

- â± **Delayed Processing**  
   - If the consumer is behind, messages are not processed in real-time.  
   - Example: In `an e-commerce system`, delayed order processing can lead to unhappy customers.

- ğŸ’¾ **Memory & Storage Pressure**  
   - Kafka stores messages until they are read or expired.
   - `If consumers are slow`, the `topic may hold many unprocessed messages`, `increasing storage usage`.

- âš¡ **System Instability**  
  - Very high lag can indicate system problems:
    - ğŸ’¥ Consumers crashed or slow
    - ğŸ”¢ Too few consumers for the number of partitions
    - ğŸŒ Network or resource bottlenecks

- ğŸ—‘ **Data Loss Risk (if retention is short)**  
  - Kafka deletes messages after a certain retention period.
  - If the consumer hasnâ€™t read messages before they are deleted, data could be lost.


## ğŸ” What Causes the Problem?

- ğŸ¢ Slow consumer processing (e.g., heavy computation per message)
- ğŸŒ Network issues between consumer and Kafka
- ğŸ‘¥ Too few consumers for the number of partitions
- âš¡ High message production rate

Consumer lag is `a sign that your system` is `â€œfalling behindâ€` and `might lead to delays` or `data loss`.


## ğŸ’¡ Solutions

### 1ï¸âƒ£ Make Consumers Faster
If your consumer is slow, it canâ€™t keep up. Ways to make it faster:

- âš™ï¸ **Optimize processing logic:**  
  - Process messages faster or do less work per message.  
  - Example: Instead of saving to a database one by one, batch writes.

- ğŸ”„ **Use asynchronous processing:**  
  - Donâ€™t block the consumer while waiting for slow operations.

- ğŸ–¥ **Upgrade hardware or resources:**  
  - `More CPU, RAM`, or `faster disk` can help `your consumer keep up`.

### 2ï¸âƒ£ Increase the Number of Consumers
Kafka allows multiple consumers in a consumer group to share partitions.

- ğŸ§© Topic has 4 partitions.
- ğŸ‘¤ Only 1 consumer â†’ reads 1 partition at a time â†’ lag builds.
- â• Add 3 more consumers â†’ each reads a partition â†’ lag decreases.

**Rule:** Number of consumers â‰¤ number of partitions.

### 3ï¸âƒ£ Tune Kafka Configuration
Some Kafka settings can help reduce lag:

- ğŸ“¥ `fetch.min.bytes / fetch.max.wait.ms`: Control how consumers fetch messages (batch more efficiently).
- ğŸ“„ `max.poll.records`: Increase the number of messages fetched in one poll to reduce lag.
- â² `session.timeout.ms / heartbeat.interval.ms`: Ensure consumers stay connected properly to avoid unnecessary rebalancing.

### 4ï¸âƒ£ Handle Backpressure
Sometimes producers are too fast. Options:

- ğŸŒ **Throttle producers:** Slow down message production if consumers canâ€™t keep up.
- ğŸ—‚ **Buffer messages in a queue:** Temporarily store messages for consumers to catch up.

### 5ï¸âƒ£ Monitor Lag
Always track consumer lag using tools like:

- ğŸ“Š Kafkaâ€™s Consumer Group command (`kafka-consumer-groups.sh`)
- ğŸ“ˆ Monitoring tools (Prometheus + Grafana)

**Alerts help you act before lag becomes a serious problem.**



---

# âš™ï¸ Auto Offset Reset

- When `a consumer starts reading a topic`, `it needs to know` `where to start reading messages from`. 
- `Kafka stores the offset` of the `last message a consumer read`. 
- `If the consumer has no saved offset` (first time reading) or the `saved offset is no longer valid` (for example, if the data has been deleted because itâ€™s older than the retention period),  
- Kafka uses **auto.offset.reset** to decide what to do.

- By default, Kafka (and Spring Kafka) sets:
  - `ğŸ§­ auto.offset.reset = latest`

## ğŸ§­ It has two main options:

- â®ï¸ **earliest** â†’ Start reading from the oldest message in the topic.
- â­ï¸ **latest** â†’ Start reading from the new messages arriving from now on.

### ğŸ“‹ Setting Behavior

| âš™ï¸ Setting | ğŸ§  Behavior |
|------------|-------------|
| earliest | Reads all messages from the beginning of the topic if no offset exists |
| latest | Reads only new messages that arrive after the consumer starts |
| none | Throws an error if there is no previous offset (useful for strict systems) |

## â“ Why Do We Need It?

- ğŸ†• When `a new consumer joins a group`, there is no previous offset.
- ğŸ—‘ï¸ Or `if offsets are deleted` (due to retention policies).
- ğŸ¤” Kafka needs to know where to start; otherwise, it will throw an error.


## ğŸ• When to Use

| ğŸ“Œ Situation | âœ… Recommended Setting |
|--------------|------------------------|
| First time reading a topic and `want all messages` | earliest |
| First time reading a topic and `only want new messages `| latest |
| You donâ€™t know if old messages are relevant | Usually latest |

## âš ï¸ Problems with Auto Offset Reset

- âš¡ **Data loss risk**  
   - If `latest` is used and `consumer starts after messages are produced` â†’ old messages are missed.

- ğŸ” **Reprocessing old data**  
  - If `earliest` is used â†’ consumer may `re-read old messages`, possibly `causing duplicate processing`.

- ğŸ˜• **Confusion in production**  
   - Choosing the wrong reset policy can lead to unexpected behavior in pipelines.

## ğŸ§© Solutions / Best Practices

- ğŸ¯ **Pick the right policy for your use case:**
  - `earliest` â†’ when `you need all historical data`
  - `latest` â†’ when you `only care about new incoming data`
- ğŸ§  **Manually manage offsets if you need fine control.**
- ğŸ“Š **Combine with monitoring** â†’ check consumer lag to ensure no messages are skipped or duplicated.
- ğŸ—ƒ **Use compacted topics** if reprocessing old messages frequently causes problems.

## Example

Imagine a topic `order-topic`:

| ğŸ§¾ Offset | ğŸ“¦ Message |
|------------|------------|
| 0 | order1 |
| 1 | order2 |
| 2 | order3 |
| 3 | order4 |
| 4 | order5 |

### ğŸ”¹ Scenario 1: Consumer has no offset

- âš™ï¸ **auto.offset.reset = earliest** â†’ starts at offset **0** (order1)
- âš™ï¸ **auto.offset.reset = latest** â†’ starts at offset **5** (new messages only)

### ğŸ”¹ Scenario 2: Consumer has offset 2

- â–¶ï¸ It continues reading from offset **3** (normal behavior, auto offset reset is not used here)

### âš™ï¸ When is `auto.offset.reset` Used?

`auto.offset.reset` is used when Kafka **cannot find a valid committed offset** for a consumer group and partition  

for example:
- ğŸ†• A new consumer group starts reading a topic for the first time (no offset yet).
- ğŸ—‘ï¸ The stored offset in **`__consumer_offsets`** has expired or been deleted due to offset retention settings or log cleanup.

### ğŸ“Œ Otherwise

- If a valid offset exists in **`__consumer_offsets`**,  
- Kafka uses that offset and **does not apply** `auto.offset.reset`.


## âš™ï¸ Kafka Consumer Configuration

``` bash

spring:
  kafka:
    consumer:
      # ğŸ” Auto offset reset (decide where to start if no offset is found)
      # ğŸ§­ Options: earliest | latest | none
      auto-offset-reset: earliest

```

## ğŸ§¾ Summary

- âš™ï¸ Auto Offset Reset decides where a consumer starts reading if no valid offset exists.
- ğŸ§­ Options: earliest (oldest) / latest (new).
- âš–ï¸ Use it carefully depending on whether you want all past messages or only new messages.
- âš ï¸ Problems: data loss or duplicate processing.
- ğŸ’¡ Solution: choose policy wisely, monitor consumers, or manage offsets manually.


---

# ğŸš€ Producer-Side ACKs in Apache Kafka

When a producer sends a message to Kafka, it can ask for different levels of acknowledgment (confirmation) from the broker (Kafka server).  
This setting is controlled by the **`acks`** parameter.

There are 3 main types:

## 1ï¸âƒ£ acks = 0
- Producer `doesnâ€™t wait for any acknowledgment`.  
- It just `sends the message` and `moves on`.

**âœ… Advantage:**
- Super fast (low latency).

**âŒ Problem:**
- If `the broker crashes` or `network fails`, `messages can be lost` because the `producer never checks` if they arrived.

**ğŸ’¡ Solution:**
- Use only when you can tolerate message loss (e.g., logs, metrics).
- Otherwise, use **acks=1** or **acks=all** for reliability.

**ğŸ“˜ Real-life Use Cases:**
- ğŸ“ `Collecting application logs` where losing a few entries is fine.
- ğŸŒ `Sending IoT sensor data continuously`, where speed matters more than 100% delivery.
- ğŸ“Š `Website analytics hits` (e.g., page view counts) that `donâ€™t need perfect accuracy`.

## 2ï¸âƒ£ acks = 1

- Producer waits for **leader broker** (the main broker for that partition) to confirm the message is written.

**âœ… Advantage:**
- Balance between speed and safety.
- The message is written **at least once** to the leader.

**âŒ Problem:**
- If the `leader crashes before followers replicate the data`, that `message can be lost` (since only the leader had it).

**ğŸ’¡ Solution:**
- Use this when small data loss is acceptable, or increase replication and use **acks=all** for higher durability.

**ğŸ“˜ Real-life Use Cases:**
- ğŸ‘¤ `Sending user activity events` (likes, clicks, scrolls) on `a website` or app or `Social Media App`.
- ğŸ“‚ `Log aggregation systems` where most data is retained but occasional loss is acceptable.
- ğŸ“ˆ `Streaming metrics` (like CPU usage, app performance) where timeliness > perfection.

## 3ï¸âƒ£ acks = all (or -1)
 
- Producer waits until `all in-sync replicas (ISR)` confirm they got the message.

**âœ… Advantage:**
- Most reliable â€” message is safe even if leader crashes.

**âŒ Problem:**
- `Slower` because `producer must wait for all replicas to confirm`.
- If `replicas are slow`, `throughput drops`.

**ğŸ’¡ Solution:**
- Use this when **data must never be lost** (e.g., financial transactions).
- To improve performance, ensure replicas are healthy and close in the network.

**ğŸ“˜ Real-life Use Cases:**
- ğŸ’³ `Bank transactions` or payment messages â€” every record must be stored safely.
- ğŸ›’ `E-commerce orders` â€” you canâ€™t lose an order even if servers crash.
- ğŸ“¦ `Inventory or stock updates` â€” consistency is critical across systems.

### Kafka Producer Configuration âš¡

Configure Kafka Producer Properties in `application.yml`.
- `acks` can be set to `0`, `1`, or `all`:

```bash

application.yaml (only acks)

spring:
  kafka:
    producer:
      acks: all
```

| âš¡ ACK Value | ğŸ“ Description                       |
|-------------|-------------------------------------|
| 0           | ğŸš€ No wait, fastest but risky       |
| 1           | â³ Wait for leader only              |
| all         | ğŸ”’ Wait for all in-sync replicas, safest |


```bash

application.yaml ( acks + Retries and Idempotence ğŸ”„)

spring:
  kafka:
    producer:
      acks: all
      retries: 3
      enable-idempotence: true
```

## Key Notes ğŸ’¡

- âš™ï¸ `acks is set in producer configuration`; Spring Kafka passes it to the Kafka client.
- ğŸ”„ `Retries + Idempotence` help `avoid duplicate messages` when using acks=all or acks=1.
- ğŸ›¡ï¸ `Critical Data`: Always use acks=all + retries>0 + enable-idempotence=true.
- âš¡ `Non-critical Logs`: acks=0 is enough for speed.
- ğŸ”„ `Retries + multiple in-flight messages` = possible reordering.
  - âš™ï¸ Set `max.in.flight.requests.per.connection=1` â†’ `ensures strict ordering`.
    - ğŸ“ This setting tells the producer: `only send 1 message` at a time per connection.
    - â³ That way, Kafka wonâ€™t send the next message until the previous one is fully acknowledged.
    - âœ… Result: order is preserved, even with retries.
    - âš ï¸ Effect:
      - ğŸ•’ If the broker is slow or thereâ€™s a network delay, all following messages are blocked, increasing latency.
    - â“ Why this matters
      - ğŸ’° Strict ordering is often required for financial transactions, logs, or inventory updates.
      - âš¡ But if you have high-throughput use cases, like metrics or clickstream events, this can significantly reduce performance.

### Tradeoff âš–ï¸:

| âš™ï¸ Setting | ğŸ‘ Pros | âš ï¸ Cons | ğŸ·ï¸ Real-life Use Cases | 
|------------|---------|---------|-----------------------| 
| `max.in.flight.requests.per.connection=1` | âœ… Guarantees ordering, safe with retries | âš ï¸ Slower throughput, higher latency | 1. Banking transactions â€“ debit/credit must happen in exact order<br>2. Inventory updates â€“ prevent overselling products<br>3. Financial trade processing â€“ orders executed in sequence | 
| `max.in.flight.requests.per.connection > 1` | âš¡ Higher throughput | ğŸ”„ Messages may be reordered on retries | 1. Clickstream analytics â€“ occasional reordering doesnâ€™t break insights<br>2. Metrics collection â€“ minor ordering issues are acceptable<br>3. Log aggregation â€“ order of log entries isnâ€™t critical |


### ğŸ“Š Summary Table

| âš™ï¸ acks | â± What it waits for | âœ… Pros | âŒ Cons | ğŸ“ When to use | ğŸ“˜ Real-life Examples |
|---------|------------------|---------|---------|----------------|---------------------|
| 0       | No one            | Fastest | Data loss possible | Non-critical logs, metrics | 1. App logs<br>2. IoT data<br>3. Page views |
| 1       | Leader only       | Balanced | Some data loss possible | Most common use | 1. User clicks<br>2. Aggregated logs<br>3. Metrics |
| all / -1 | All replicas     | Most reliable | Slowest | Critical data | 1. Bank payments<br>2. Orders<br>3. Inventory updates |

---

### 1ï¸âƒ£ Role of the key in Kafka

- ğŸ—ï¸ Kafka partitions messages using the key.
- ğŸ“¦ All messages with the same key always go to the same partition.
- ğŸ“ Within a single partition, Kafka stores messages in the order they arrive.
- âœ… So yes, sending a key ensures messages for that key stay in the same partition, which is required for ordering per key.

### 2ï¸âƒ£ Why key alone doesnâ€™t guarantee order with retries

- âš ï¸ Even with the key:
  - If retries are enabled and multiple messages are in-flight:
    - ğŸ”„ Message 1 fails and is retried
    - â© Message 2 succeeds immediately
      â†’ Kafka may store them in the wrong order within the partition.

- ğŸ›¡ï¸ This is why `max.in.flight.requests.per.connection=1` is needed to strictly preserve order, even with retries, for messages with the same key.


---

## What is ISR? âš¡

- ğŸ—ï¸ `ISR = In-Sync Replicas`
- ğŸ“¦ Kafka replicates data across multiple brokers for reliability.
- ğŸ“ A topic partition has one leader and one or more followers.
- âœ… `ISR` is the set of replicas that are fully caught up with the leader (i.e., have all messages the leader has).

### Types of ISR ğŸ·ï¸

While `ISR` is essentially the list of in-sync replicas, we can think in terms of replica states:

- ğŸ‘‘ **Leader** â€“ The replica handling all read/write requests.
- âœ”ï¸ **Follower in ISR** â€“ Fully caught up, considered safe.
- âš ï¸ **Follower out of ISR** â€“ Lagging behind, not fully synced; may be temporarily removed from ISR until it catches up.


### Problem: How to ensure data reliability and consistency when brokers fail? â“

- Without `ISR`, a leader could fail, and followers may not have all data, leading to data loss.
- `ISR` ensures Kafka only considers replicas fully in sync when electing a new leader or committing messages.


### How ISR Solves the Problem ğŸ›¡ï¸

- Kafka uses `ACKs` from ISR replicas for message durability.
- `acks=all` waits for all ISR replicas to confirm before considering a message committed.
- If a leader fails, only a replica in ISR can become the new leader â†’ ensures no committed data is lost.
- Followers lagging behind are temporarily removed from ISR until they catch up.

**Example:**

- Partition `R1` (leader), `R2` & `R3` (followers)
- `R2` and `R3` are in `ISR`
- Leader receives a write â†’ waits for ISR replicas to acknowledge â†’ commits
- If `R1` crashes, only `R2` or `R3` (in ISR) can become the new leader â†’ no committed data loss


### Real-life Use Case ğŸ’¡

- **E-commerce orders processing**:  
  - Partition for orders topic replicated across 3 brokers.  
  - `ISR` ensures all replicas have the same order data.  
  - If one broker fails, Kafka can elect a new leader from ISR â†’ no orders are lost.

- **Financial transactions**:  
  - `ISR` ensures committed trades are never lost even if a broker fails.

- **Logging / Metrics pipelines**:  
  - Guarantees log events are replicated and available across multiple brokers.

### Problem: Follower Out of ISR âš ï¸

In Kafka, a follower replica is supposed to stay in sync with the leader.  
Sometimes, a follower lags behind due to:

- ğŸŒ Slow network
- ğŸ–¥ï¸ High load on the broker
- ğŸ’¾ Disk or CPU bottlenecks

**Effect:**

1. â³ **Temporary removal from ISR**:  
   - The leader of the partition monitors followers. If a follower falls behind beyond the allowed timeout (`replica.lag.time.max.ms`), the leader removes it from ISR until it catches up.

2. ğŸš« **Cannot serve as leader**:  
   - Followers outside ISR cannot be elected as leader, ensuring no data loss.

3. ğŸ¢ **May delay message commits if `acks=all`**:  
   - If your producer is configured with `acks=all` (wait for all ISR replicas to confirm a message), a smaller ISR may slow down message commits because the leader waits for fewer replicas.  
   - If too many followers are out of ISR, this can reduce redundancy and affect durability guarantees.


### How to Overcome Lagging Followers ğŸ› ï¸

**Practical solutions:**

- ğŸŒ **Check Network / Broker Performance**  
  - Ensure fast and reliable network between brokers.  
  - Avoid overloaded brokers or slow disks.

- âš¡ **Increase Replication Performance**  
  - Tune Kafka configs for followers:
    - `replica.fetch.max.bytes` â†’ allow followers to fetch more messages per request
    - `replica.fetch.wait.max.ms` â†’ reduce wait time for fetch

- ğŸ–¥ï¸ **Add More Resources**  
  - Give lagging brokers more CPU, RAM, or disk speed.  
  - Sometimes followers lag because they are resource-starved.

- ğŸ“Š **Reduce Leader Load**  
  - If leader is too busy, followers cannot keep up.  
  - Consider adding partitions or balancing data across brokers.

- ğŸ” **Monitor ISR**  
  - Use Kafka tools or monitoring (Prometheus/Grafana) to detect lagging replicas early.  
  - React before followers fall far behind.

### Implementation âš™ï¸

1. 1ï¸âƒ£ **Automatic Management**

   - ğŸ¤– Kafka automatically maintains `ISR` for each partition. 
   - ğŸ‘€ Leader monitors followers and removes lagging replicas temporarily.

2. 2ï¸âƒ£ **What You Control**

   - ğŸ·ï¸ `Replication factor`: Number of replicas for a topic. 
   - ğŸ“© `Producer acks`: Determines how many ISR replicas must confirm a message.

      - `acks=0` â†’ no wait
      - `acks=1` â†’ wait for leader only
      - `acks=all` â†’ wait for all ISR replicas

3. 3ï¸âƒ£ **What You Donâ€™t Need to Do**

   - ğŸš« No manual adding/removing replicas from `ISR`. 
   - ğŸ”„ Kafka handles catching up and rejoining lagging replicas automatically.


### Pros âœ…

- ğŸ›¡ï¸ **Data durability**: Only replicas that are fully in sync with the leader are considered part of ISR, ensuring that committed messages are not lost.
- ğŸ‘‘ **Safe leader election**: If the leader fails, only in-sync replicas can become the new leader, which prevents data loss.
- âš¡ **High availability**: Even if some followers fall behind, Kafka can continue processing messages using the remaining ISR replicas.
- ğŸ”„ **Automatic recovery**: Lagging followers can rejoin the ISR once they catch up, making replication resilient and self-healing.

### Cons âŒ

- ğŸŒ **Lagging followers can reduce redundancy**: When replicas fall out of ISR, fewer replicas store the committed messages, slightly increasing risk if the leader fails.
- ğŸš« **Temporary unavailability for leader election**: Lagging followers cannot become leader until fully synced, which may limit options in case of leader failure.
- â³ **Potential commit delays**: In some cases (with `acks=all`), if ISR shrinks due to slow followers, it may affect how replication guarantees behave.
- ğŸ–¥ï¸ **Resource sensitivity**: Maintaining ISR requires network, CPU, and disk performance; if brokers are slow, replicas may fall out of ISR frequently.



---
Exactly-Once Delivery  (Kafka Transactions)

---
Exactly-once vs at-least-once

---

Replica reads and consistency tradeoffs

---

Idempotent & Retryable Processing

---
Dead Letter Queues (DLQ)

When a message fails processing repeatedly, you can:

Skip it temporarily (log and retry later)

Send it to a DLQ topic to analyze and fix later

This prevents a bad message from blocking the consumer

---
Security in Consumers

Kafka supports:

TLS encryption

SASL authentication (e.g., SCRAM, GSSAPI)

ACLs for topic-level access control

Configure in consumer with:

---
Kafka Consumer Metrics

Kafka consumer exposes metrics you can monitor:

records-consumed-rate

fetch-latency-avg

commit-latency-avg

records-lag

records-lag-max

Use Prometheus + Grafana, or JMX exporters to track these.

---