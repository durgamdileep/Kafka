# ğŸ› ï¸ Step-by-Step Setup in Spring Boot

```bash

application.properties
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=my-group
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.enable-auto-commit=false

// This disables auto commit, which is necessary for manual offset management.


Create a Configuration Class

@Configuration
public class KafkaConsumerConfig {

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory(
            ConsumerFactory<String, String> consumerFactory) {

        ConcurrentKafkaListenerContainerFactory<String, String> factory =
            new ConcurrentKafkaListenerContainerFactory<>();

        factory.setConsumerFactory(consumerFactory);

        // ğŸ‘‡ Manual ack mode is required for manual commits
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);

        return factory;
    }
}

Use @KafkaListener with Acknowledgment

@KafkaListener(topics = "my-topic", containerFactory = "kafkaListenerContainerFactory")
public void listen(ConsumerRecord<String, String> record, Acknowledgment ack) {
    try {
        // Process message
        System.out.println("Received: " + record.value());

        // Business logic (e.g., DB write)

        // âœ… Commit the offset manually
        ack.acknowledge();
    } catch (Exception e) {
        // Optionally handle errors, retry, etc.
    }
}

/** 
Why You Need kafkaListenerContainerFactory

Spring Bootâ€™s default Kafka listener container uses AckMode.BATCH or AckMode.RECORD, and auto-commit behavior, depending on the properties.
To use `MANUAL commit` or `MANUAL_IMMEDIATE` , you must override the container factory and set
*/
```

### AckMode.BATCH setup in Kafka

- ğŸ“¦ `max.poll.records` controls batch size and enable `batchListener = true` in kafkaListenerContainerFactory
- ğŸ¯ `@KafkaListener` with `List<T>` parameter receives messages as a batch.
- ğŸ”’ `enable-auto-commit=false + AckMode.BATCH` ensures `offsets are committed automatically` after `each batch is processed`;
- the `developer does not call ack.acknowledge() manually`.
- âš¡ This is still streaming under the hood, but you are processing messages in batches.
- â— This is **not traditional batch processing**; it is just a batch in Kafka.

### Difference: Traditional Batch vs Kafka Batch

| âš¡ Aspect | ğŸ¢ Traditional Batch | ğŸª¶ Kafka Batch |
|-----------|--------------------|----------------|
| ğŸ—‚ï¸ Processing | Entire dataset processed together | Messages processed continuously but delivered in small batches |
| â° Timing | Scheduled or periodic | Streaming, happens as messages arrive |
| ğŸ›ï¸ Developer Control | Usually controls when batch starts and ends | Kafka controls batch delivery based on `max.poll.records` |
| âœ… Acknowledgment | Typically done after batch completes | `AckMode.BATCH` commits offsets after each batch automatically; no manual ack needed |
| ğŸ—ï¸ Nature | Blocking, all-or-nothing | Non-blocking, still streaming under the hood |
| ğŸ“Œ Use Case | Reports, ETL, large offline jobs | High-throughput message processing with batching convenience |
| ğŸ’¡ Examples | Can be done by Hadoop, Spark | Kafka only |


### 1. â“ Can we use `AckMode.BATCH` for manual commit?

ğŸ‘‰ **No**, `AckMode.BATCH` and `AckMode.RECORD` are used for **automatic offset commits** (either by `Kafka itself or Spring`, depending on settings).  
They are **not intended** for manual control of offset commits.

ğŸ› ï¸ If you want to manually commit, you must use one of these modes:

- âœ‹ `MANUAL`
- âš¡ `MANUAL_IMMEDIATE`

âœ… So: You **cannot** use `BATCH` for manual offset commit.

---

| âš™ï¸ Config                                                                | ğŸ“ Description                                                                                                                |
|--------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| `enable-auto-commit=true`                                                | ğŸ”„ `Kafka auto-commits` offsets at intervals (`auto.commit.interval.ms`) â€” Spring's AckMode is ignored.                       |
| `enable-auto-commit=false` + `AckMode.BATCH`                             | ğŸ”§ `Spring Kafka commits` offsets after each batch is processed â€” this is `not "manual commit"` but Spring-controlled commit. |
| `enable-auto-commit=false` + `AckMode.MANUAL`/`AckMode.MANUAL_IMMEDIATE` | âš™ï¸ you have to commit the offset yourself                                                                                     |
---

### 2. â“ What happens if consumer crashes before batch size is reached (e.g. AckMode.BATCH)?

- âš™ï¸ If you're using `AckMode.BATCH` with auto-commit disabled , and:
    - ğŸ› ï¸ You process some messages
    - ğŸ’¥ A crash happens before the batch is acknowledged

- âš ï¸ Then:
    - â— Offsets are **not committed** â€” those messages will be reprocessed after restart.

âš™ï¸ This is **by design** â€” batching delays offset commits to improve performance, but at the cost of reliability if the consumer fails mid-batch.

---

| âš™ï¸ **enable-auto-commit** | ğŸ› ï¸ **AckMode**               | ğŸ‘¤ **Who commits the offset?**       | â° **When?**                                    |
|--------------------------|------------------------------|-------------------------------------|------------------------------------------------|
| âœ… `true`                 | ğŸš« Ignored                   | ğŸ¤– Kafka client                    | â²ï¸ Every interval (`auto.commit.interval.ms`)  |
| âŒ `false`                | ğŸ“¦ `AckMode.BATCH`            | ğŸŒ± Spring                          | ğŸ“„ After each batch is processed                |
| âŒ `false`                | ğŸ“ƒ `AckMode.RECORD`           | ğŸŒ± Spring                          | ğŸ“ After each record is processed               |
| âŒ `false`                | âœ‹ `MANUAL`                   | ğŸ‘¨â€ğŸ’» You (your code)                | ğŸ–±ï¸ When you call `ack.acknowledge()`           |

- âœ… So `AckMode.BATCH` / `AckMode.RECORD` works **only** when `enable-auto-commit = false`, Spring Kafka will commit the offset.
- âŒ If `enable-auto-commit = true`, then `AckMode` is ignored â€” Kafka client commits the offset.
- âš™ï¸ `enable-auto-commit=false`, and `AckMode.MANUAL` / `AckMode.MANUAL_IMMEDIATE`, you have to commit the offset yourself.


---

### 3.â“ If I use manual offset commit (`AckMode.MANUAL`), will it lead to more network calls to Kafka, and is that a performance issue?

ğŸ”¹ **Yes** â€” if you commit after every message, manual commit causes more network calls to Kafka.

Because:

- ğŸ“ Each call to `ack.acknowledge()` triggers an offset commit request to Kafka.
- ğŸŒ Thatâ€™s a separate network call.
- âš¡ So if you process 1,000 messages per second and acknowledge every message â†’ thatâ€™s up to **1,000 network calls per second** just for commits.

---

### ğŸ§  Why Itâ€™s a Problem

- ğŸ“¡ **Network Overhead:** Too many small commit requests.
- ğŸ‹ï¸ **Broker Load:** Kafka brokers handle more commit requests.
- â³ **Throughput Impact:** Your app spends more time waiting on network.

---

### âœ… Solution: Manual + Batching

Even with `AckMode.MANUAL`, you can reduce network calls by:

ğŸ‘‰ **Manually committing in batches:**
```bash

    int count = 0;
    List<String> buffer = new ArrayList<>();
    
    @KafkaListener(...)
    public void listen(String message, Acknowledgment ack) {
        buffer.add(message);
        count++;
    
        if (count >= 10) {
            process(buffer);         // Your logic
            ack.acknowledge();       // One commit for 10 messages
            buffer.clear();
            count = 0;
        }
    }
```

âœ… This gives you:

- ğŸ›ï¸ Full control
- ğŸ“‰ Fewer network calls
- âš–ï¸ Balanced performance

---
### 4. In Kafka ğŸŸ¡, if a consumer ğŸ‘¤ is using manual offset management ğŸ“Š with batch commits ğŸ—‚ï¸, how does it fetch new messages ğŸ“¥ without committing the current offset ğŸ”’, and what happens to offsets if the consumer crashes ğŸ›‘ or stays active â–¶ï¸?

**Answer:**

- The `consumer keeps track of` `offsets` `in memory` ğŸ§ , so it `can fetch new messages` ğŸ“¥ even without committing. 
- If the consumer crashes ğŸ›‘, it uses the `last committed offset` ğŸ”’ `from the broker`. 
- If the consumer is still running â–¶ï¸, it `uses the in-memory offset` ğŸ§ .
- `Developers donâ€™t need to create` the `in-memory storage`; Kafka does it automatically âš¡.

---
### âœ… Final Summary

| ğŸ§¾ Commit Style         | ğŸ“¶ Network Calls | âœ”ï¸ Reliable? | ğŸ›ï¸ Control    |
|------------------------|------------------|--------------|---------------|
| Manual (per message)   | ğŸ”º High          | âœ… Yes       | âœ… Full       |
| Manual (batched)       | ğŸ”» Low           | âœ… Yes       | âœ… Full       |
| Spring auto (`BATCH`)  | ğŸ”» Low           | âœ… Yes       | âŒ No        |
| Kafka auto-commit      | ğŸ”» Low           | âŒ Risky     | âŒ None      |

---

### Kafka Consumer Offset Commit Behavior

| # | Offset Commit Timing | Message Processing Status | Commit Stored in Broker? | Commit ACK Received by Consumer? | Consumer Restart Behavior | Outcome on Message | Delivery Guarantee / Behavior |
|---|--------------------|--------------------------|-------------------------|---------------------------------|--------------------------|------------------|------------------------------|
| 1 | After processing   | âœ… Success               | ğŸ”’ Yes                  | âœ… Yes                          | `Starts from next offset â†’ last committed offset = 4 â†’ starts from 5` | Processed once, no reprocess ğŸ”„ | At-least-once â˜‘ï¸ |
| 2 | After processing   | âœ… Success               | ğŸ”’ Yes                  | âŒ No                           | `Starts from next offset â†’ last committed offset = 4 â†’ starts from 5` | Processed once, no reprocess ğŸ”„ | At-least-once â˜‘ï¸ |
| 3 | After processing   | âŒ Failed               | âŒ No                   | â€”                               | `Starts from next offset â†’ last committed offset = 3 â†’ starts from 3` | Message reprocessed ğŸ”„ | At-least-once â˜‘ï¸ |
| 4 | After processing   | âŒ Failed               | âŒ No                   | â€”                               | `Starts from next offset â†’ last committed offset = 3 â†’ starts from 3` | Message reprocessed ğŸ”„ | At-least-once â˜‘ï¸ |
| 5 | Before processing  | âœ… Success               | ğŸ”’ Yes                  | âœ… Yes                          | `Starts from next offset â†’ last committed offset = 4 â†’ starts from 5` | Message processed ğŸ“¬ | At-most-once âŒ |
| 6 | Before processing  | âœ… Success               | ğŸ”’ Yes                  | âŒ No                           | `Starts from next offset â†’ last committed offset = 4 â†’ starts from 5` | Message processed ğŸ“¬ | At-most-once âŒ |
| 7 | Before processing  | ğŸ›‘ Crash before processing | ğŸ”’ Yes                 | â€”                               | `Starts from next offset â†’ last committed offset = 3 â†’ starts from 4` | Message lost âš ï¸ | At-most-once âŒ |
| 8 | After processing   | âœ… Success               | âŒ No                   | âŒ No                           | `Starts from next offset â†’ last committed offset = 3 â†’ starts from 3` | Message reprocessed ğŸ”„ | At-least-once â˜‘ï¸ (duplicates possible âš ï¸) |
| 9 | Commit + message output in transaction (EOS) | âœ… Success | ğŸ”’ Yes | â€” | `Starts from next offset â†’ transaction committed â†’ next offset = 5` | No duplicates, no loss ğŸ’ | Exactly-once ğŸ’ |

---

# ğŸ§‘â€ğŸ’» Offset Commit Control in Spring Kafka 

ğŸ§‘â€ğŸ’» In Spring Boot with Spring Kafka, offset commits are usually handled via `Acknowledgment.acknowledge()`, which is a synchronous commit behind the scenes. But if you want more control to explicitly choose between `commitSync()` and `commitAsync()` like in the raw Kafka consumer:

## 1. ğŸŸ¢ Default in Spring Kafka (`ack.acknowledge()`)

- Calling `acknowledge()` triggers a synchronous commit â€” Spring waits for the commit to complete (similar to `commitSync()`).
- âš™ï¸ Itâ€™s simple and reliable for most use cases.

## 2. âš¡ How to use `commitAsync()` in Spring Kafka?


- Spring Kafka doesnâ€™t expose `commitAsync()` directly on the `Acknowledgment` interface, but you can do it manually by accessing the underlying Kafka consumer.
- Using `commitAsync()` manually inside `@KafkaListener`
   - To get the `Consumer<?, ?>` injected, your listener method should include it as a parameter.
   - ğŸ§® You must calculate offsets manually and call `commitAsync()` on the consumer instance.
   - âš ï¸ This approach is lower-level, bypassing Spring Kafkaâ€™s `Acknowledgment` abstraction.
   - ğŸš« You lose some container-managed features like retries and error handling integration.

### ğŸ“‹ Best practice in Spring Kafka projects

- âœ… Use safe synchronous commit (`acknowledge()`).
- âš¡ Use manual `commitAsync()` on the `Kafka consumer only` if you need high throughput and can tolerate potential re-processing.
- âš ï¸ Always handle commit failures in the callback when using `commitAsync()`.

---

## Example in Java
```bash

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Arrays.asList("my-topic"));

while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    
    for (ConsumerRecord<String, String> record : records) {
        // 1. Process the record
        System.out.printf("Consumed record: key = %s, value = %s%n", record.key(), record.value());
        
        // 2. Do something meaningful (e.g., write to DB)
    }
    
    // 3. Commit offsets manually AFTER processing
    consumer.commitSync();  // or commitAsync() 
    
    // or commitAsync with callback handle failure
      consumer.commitAsync((offsets, exception) -> {
            if (exception != null) {
                System.err.println("Commit failed: " + exception.getMessage());
            }
        });
}

```